{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import cv2  # Assuming you have OpenCV installed\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import math\n",
    "from vit_pytorch.efficient import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "dataset = \"C:\\\\Study\\\\OJT\\\\Dataset\\\\UCF50\"          # Dataset path\n",
    "dataset2 = \"C:\\\\Study\\\\OJT\\\\Dataset_Extraction\\\\UCF50\\\\Full\"                # Dataset2 path\n",
    "train_path = \"C:\\\\Study\\\\OJT\\\\Dataset_Extraction\\\\UCF50\\\\UCF50Train\"             # Training path\n",
    "test_path = \"C:\\\\Study\\\\OJT\\\\Dataset_Extraction\\\\UCF50\\\\UCF50Test\"             # Testing path\n",
    "no_of_frames = 10000                     # Total number of frames to be extracted\n",
    "categories = os.listdir(dataset)        # Name of each class/category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder C:\\Study\\OJT\\Dataset_Extraction\\UCF50\\Full already exists...\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset directory\n",
    "try:\n",
    "    os.mkdir(dataset2)\n",
    "    print(\"Folder {} created...\".format(dataset2))\n",
    "except:\n",
    "    print(\"A folder {} already exists...\".format(dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder C:\\Study\\OJT\\Dataset_Extraction\\UCF50\\UCF50Train already exists...\n"
     ]
    }
   ],
   "source": [
    "# Creating training_set directory\n",
    "try:\n",
    "    os.mkdir(train_path)\n",
    "    print(\"Folder {} created...\".format(train_path))\n",
    "except:\n",
    "    print(\"A folder {} already exists...\".format(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder C:\\Study\\OJT\\Dataset_Extraction\\UCF50\\UCF50Test already exists...\n"
     ]
    }
   ],
   "source": [
    "# Creating testing_set directory\n",
    "try:\n",
    "    os.mkdir(test_path)\n",
    "    print(\"Folder {} created...\".format(test_path))\n",
    "except:\n",
    "    print(\"A folder {} already exists...\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder BaseballPitch created...\n",
      "Folder Basketball created...\n",
      "Folder BenchPress created...\n",
      "Folder Biking created...\n",
      "Folder Billiards created...\n",
      "Folder BreastStroke created...\n",
      "Folder CleanAndJerk created...\n",
      "Folder Diving created...\n",
      "Folder Drumming created...\n",
      "Folder Fencing created...\n",
      "Folder GolfSwing created...\n",
      "Folder HighJump created...\n",
      "Folder HorseRace created...\n",
      "Folder HorseRiding created...\n",
      "Folder HulaHoop created...\n",
      "Folder JavelinThrow created...\n",
      "Folder JugglingBalls created...\n",
      "Folder JumpingJack created...\n",
      "Folder JumpRope created...\n",
      "Folder Kayaking created...\n",
      "Folder Lunges created...\n",
      "Folder MilitaryParade created...\n",
      "Folder Mixing created...\n",
      "Folder Nunchucks created...\n",
      "Folder PizzaTossing created...\n",
      "Folder PlayingGuitar created...\n",
      "Folder PlayingPiano created...\n",
      "Folder PlayingTabla created...\n",
      "Folder PlayingViolin created...\n",
      "Folder PoleVault created...\n",
      "Folder PommelHorse created...\n",
      "Folder PullUps created...\n",
      "Folder Punch created...\n",
      "Folder PushUps created...\n",
      "Folder RockClimbingIndoor created...\n",
      "Folder RopeClimbing created...\n",
      "Folder Rowing created...\n",
      "Folder SalsaSpin created...\n",
      "Folder SkateBoarding created...\n",
      "Folder Skiing created...\n",
      "Folder Skijet created...\n",
      "Folder SoccerJuggling created...\n",
      "Folder Swing created...\n",
      "Folder TaiChi created...\n",
      "Folder TennisSwing created...\n",
      "Folder ThrowDiscus created...\n",
      "Folder TrampolineJumping created...\n",
      "Folder VolleyballSpiking created...\n",
      "Folder WalkingWithDog created...\n",
      "Folder YoYo created...\n"
     ]
    }
   ],
   "source": [
    "# Creating same directories for dataset2/ that are already present in the dataset directory\n",
    "for category in categories:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(dataset2, category))\n",
    "        print(\"Folder {} created...\".format(category))\n",
    "    except:\n",
    "        print(\"A folder already exists, named {}...\".format(category, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder already exists, named BaseballPitch...\n",
      "A folder already exists, named Basketball...\n",
      "A folder already exists, named BenchPress...\n",
      "A folder already exists, named Biking...\n",
      "A folder already exists, named Billiards...\n",
      "A folder already exists, named BreastStroke...\n",
      "A folder already exists, named CleanAndJerk...\n",
      "A folder already exists, named Diving...\n",
      "A folder already exists, named Drumming...\n",
      "A folder already exists, named Fencing...\n",
      "A folder already exists, named GolfSwing...\n",
      "A folder already exists, named HighJump...\n",
      "A folder already exists, named HorseRace...\n",
      "A folder already exists, named HorseRiding...\n",
      "A folder already exists, named HulaHoop...\n",
      "A folder already exists, named JavelinThrow...\n",
      "A folder already exists, named JugglingBalls...\n",
      "A folder already exists, named JumpingJack...\n",
      "A folder already exists, named JumpRope...\n",
      "A folder already exists, named Kayaking...\n",
      "A folder already exists, named Lunges...\n",
      "A folder already exists, named MilitaryParade...\n",
      "A folder already exists, named Mixing...\n",
      "A folder already exists, named Nunchucks...\n",
      "A folder already exists, named PizzaTossing...\n",
      "A folder already exists, named PlayingGuitar...\n",
      "A folder already exists, named PlayingPiano...\n",
      "A folder already exists, named PlayingTabla...\n",
      "A folder already exists, named PlayingViolin...\n",
      "A folder already exists, named PoleVault...\n",
      "A folder already exists, named PommelHorse...\n",
      "A folder already exists, named PullUps...\n",
      "A folder already exists, named Punch...\n",
      "A folder already exists, named PushUps...\n",
      "A folder already exists, named RockClimbingIndoor...\n",
      "A folder already exists, named RopeClimbing...\n",
      "A folder already exists, named Rowing...\n",
      "A folder already exists, named SalsaSpin...\n",
      "A folder already exists, named SkateBoarding...\n",
      "A folder already exists, named Skiing...\n",
      "A folder already exists, named Skijet...\n",
      "A folder already exists, named SoccerJuggling...\n",
      "A folder already exists, named Swing...\n",
      "A folder already exists, named TaiChi...\n",
      "A folder already exists, named TennisSwing...\n",
      "A folder already exists, named ThrowDiscus...\n",
      "A folder already exists, named TrampolineJumping...\n",
      "A folder already exists, named VolleyballSpiking...\n",
      "A folder already exists, named WalkingWithDog...\n",
      "A folder already exists, named YoYo...\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(train_path, category))\n",
    "        print(\"Folder {} created...\".format(category))\n",
    "    except:\n",
    "        print(\"A folder already exists, named {}...\".format(category, train_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder already exists, named BaseballPitch...\n",
      "A folder already exists, named Basketball...\n",
      "A folder already exists, named BenchPress...\n",
      "A folder already exists, named Biking...\n",
      "A folder already exists, named Billiards...\n",
      "A folder already exists, named BreastStroke...\n",
      "A folder already exists, named CleanAndJerk...\n",
      "A folder already exists, named Diving...\n",
      "A folder already exists, named Drumming...\n",
      "A folder already exists, named Fencing...\n",
      "A folder already exists, named GolfSwing...\n",
      "A folder already exists, named HighJump...\n",
      "A folder already exists, named HorseRace...\n",
      "A folder already exists, named HorseRiding...\n",
      "A folder already exists, named HulaHoop...\n",
      "A folder already exists, named JavelinThrow...\n",
      "A folder already exists, named JugglingBalls...\n",
      "A folder already exists, named JumpingJack...\n",
      "A folder already exists, named JumpRope...\n",
      "A folder already exists, named Kayaking...\n",
      "A folder already exists, named Lunges...\n",
      "A folder already exists, named MilitaryParade...\n",
      "A folder already exists, named Mixing...\n",
      "A folder already exists, named Nunchucks...\n",
      "A folder already exists, named PizzaTossing...\n",
      "A folder already exists, named PlayingGuitar...\n",
      "A folder already exists, named PlayingPiano...\n",
      "A folder already exists, named PlayingTabla...\n",
      "A folder already exists, named PlayingViolin...\n",
      "A folder already exists, named PoleVault...\n",
      "A folder already exists, named PommelHorse...\n",
      "A folder already exists, named PullUps...\n",
      "A folder already exists, named Punch...\n",
      "A folder already exists, named PushUps...\n",
      "A folder already exists, named RockClimbingIndoor...\n",
      "A folder already exists, named RopeClimbing...\n",
      "A folder already exists, named Rowing...\n",
      "A folder already exists, named SalsaSpin...\n",
      "A folder already exists, named SkateBoarding...\n",
      "A folder already exists, named Skiing...\n",
      "A folder already exists, named Skijet...\n",
      "A folder already exists, named SoccerJuggling...\n",
      "A folder already exists, named Swing...\n",
      "A folder already exists, named TaiChi...\n",
      "A folder already exists, named TennisSwing...\n",
      "A folder already exists, named ThrowDiscus...\n",
      "A folder already exists, named TrampolineJumping...\n",
      "A folder already exists, named VolleyballSpiking...\n",
      "A folder already exists, named WalkingWithDog...\n",
      "A folder already exists, named YoYo...\n"
     ]
    }
   ],
   "source": [
    "# Creating same directories for testing_set/ that are already present in the dataset directory\n",
    "for category in categories:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(test_path, category))\n",
    "        print(\"Folder {} created...\".format(category))\n",
    "    except:\n",
    "        print(\"A folder already exists, named {}...\".format(category, test_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725de5f1bd0c467baf78165f3122dfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combining multiple videos into single video file\n",
    "for category in tqdm(categories):\n",
    "    videofiles = glob.glob(os.path.join(dataset, category, \"**/*.avi\"), recursive=True)\n",
    "    if videofiles:\n",
    "        cap = cv2.VideoCapture(videofiles[0])\n",
    "    else:\n",
    "        print(\"No video files found in {}/{}\".format(dataset, category))\n",
    "    video_index = 0\n",
    "    cap = cv2.VideoCapture(videofiles[0])    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    # fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(\"{}/{}/{}.avi\".format(dataset2, category, category), fourcc, 25, (320, 240))\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if frame is None:\n",
    "            video_index += 1\n",
    "            if video_index >= len(videofiles):\n",
    "                break\n",
    "            else:\n",
    "                cap = cv2.VideoCapture(videofiles[ video_index ])\n",
    "                ret, frame = cap.read()\n",
    "                out.write(frame)\n",
    "        else:\n",
    "            out.write(frame)\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5bdc36feb0439b99bcc875719683c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14944, 18949, 18966, 32839, 42936, 18340, 21216, 26284, 46503, 14570, 23444, 13796, 32746, 38943, 13084, 11855, 30354, 10387, 64620, 30829, 32488, 15983, 16307, 35875, 13988, 39933, 23076, 34262, 18383, 26672, 40890, 16769, 43334, 8721, 56045, 21399, 45329, 29090, 16486, 31370, 22995, 45578, 23160, 18256, 26493, 12609, 24002, 12836, 27697, 23350]\n"
     ]
    }
   ],
   "source": [
    "# Saving total no. of frames of each classes/categories into an array\n",
    "total_frames = []\n",
    "for category in tqdm(categories):\n",
    "    cap = cv2.VideoCapture(dataset2 + \"/\" + category + \"/\" + category + \".avi\")\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_frames.append(length)\n",
    "    cap.release()\n",
    "    \n",
    "print(total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90eeb880f786414ab75730c59bab3458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting one frame per five frame from the Videos\n",
    "for category in tqdm(categories):\n",
    "    count = 0    \n",
    "    a = glob.glob(dataset2 + '/' + category + '/' + category +'.avi')\n",
    "    for i in range(len(a)):\n",
    "        cap = cv2.VideoCapture(a[i])\n",
    "        frameRate = cap.get(5)\n",
    "        while(cap.isOpened()):\n",
    "            frameId = cap.get(1)\n",
    "            ret, frame = cap.read()\n",
    "            if (ret != True):\n",
    "                break\n",
    "            if (frameId % math.floor(frameRate) == 0):\n",
    "                cv2.imwrite(train_path + '/' + category + '/{}_{}.jpg'.format(category, count), frame)\n",
    "                count += 1\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0a9162026d4a488b77fb134c8cab64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Moving 150 random images from training_set into testing_set\n",
    "for category in tqdm(categories):\n",
    "    sub_file = [file for file in glob.glob(train_path +'/'+ category +'/'+ \"*.jpg\")]\n",
    "    test_files = random.sample(sub_file, 25)\n",
    "    for test_file in test_files:\n",
    "        img = cv2.imread(test_file)\n",
    "        os.remove(test_file)\n",
    "        test_filename = os.path.basename(test_file)\n",
    "        cv2.imwrite(test_path +'/' + category + '/' + test_filename , img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for loading and preprocessing images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),  # You can adjust the crop size as needed\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset for the entire training set\n",
    "full_dataset = ImageFolder(train_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "val_size = len(full_dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom datasets for training and test\n",
    "train_dataset = ImageFolder(train_path, transform=transform)\n",
    "test_dataset = ImageFolder(test_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for training and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_transformer = Linformer(\n",
    "    dim=128,\n",
    "    seq_len=49+1,  # 7x7 patches + 1 cls-token\n",
    "    depth=12,\n",
    "    heads=8,\n",
    "    k=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    dim=128,\n",
    "    image_size=224,\n",
    "    patch_size=32,\n",
    "    num_classes=50,\n",
    "    transformer=efficient_transformer,\n",
    "    channels=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10/812, Loss: 3.9607, Training Accuracy: 2.66%\n",
      "Epoch 1, Batch 20/812, Loss: 3.7782, Training Accuracy: 4.30%\n",
      "Epoch 1, Batch 30/812, Loss: 3.6191, Training Accuracy: 5.78%\n",
      "Epoch 1, Batch 40/812, Loss: 3.4880, Training Accuracy: 7.58%\n",
      "Epoch 1, Batch 50/812, Loss: 3.3489, Training Accuracy: 9.06%\n",
      "Epoch 1, Batch 60/812, Loss: 3.3231, Training Accuracy: 9.87%\n",
      "Epoch 1, Batch 70/812, Loss: 3.2074, Training Accuracy: 11.03%\n",
      "Epoch 1, Batch 80/812, Loss: 3.0624, Training Accuracy: 12.05%\n",
      "Epoch 1, Batch 90/812, Loss: 2.9883, Training Accuracy: 13.32%\n",
      "Epoch 1, Batch 100/812, Loss: 2.9204, Training Accuracy: 14.47%\n",
      "Epoch 1, Batch 110/812, Loss: 2.9466, Training Accuracy: 15.28%\n",
      "Epoch 1, Batch 120/812, Loss: 2.7122, Training Accuracy: 16.39%\n",
      "Epoch 1, Batch 130/812, Loss: 2.7586, Training Accuracy: 17.31%\n",
      "Epoch 1, Batch 140/812, Loss: 2.6899, Training Accuracy: 18.20%\n",
      "Epoch 1, Batch 150/812, Loss: 2.5355, Training Accuracy: 19.28%\n",
      "Epoch 1, Batch 160/812, Loss: 2.6229, Training Accuracy: 20.10%\n",
      "Epoch 1, Batch 170/812, Loss: 2.3460, Training Accuracy: 21.25%\n",
      "Epoch 1, Batch 180/812, Loss: 2.4510, Training Accuracy: 22.01%\n",
      "Epoch 1, Batch 190/812, Loss: 2.4363, Training Accuracy: 22.68%\n",
      "Epoch 1, Batch 200/812, Loss: 2.4013, Training Accuracy: 23.48%\n",
      "Epoch 1, Batch 210/812, Loss: 2.2256, Training Accuracy: 24.41%\n",
      "Epoch 1, Batch 220/812, Loss: 2.2247, Training Accuracy: 25.22%\n",
      "Epoch 1, Batch 230/812, Loss: 2.2602, Training Accuracy: 25.84%\n",
      "Epoch 1, Batch 240/812, Loss: 2.1553, Training Accuracy: 26.59%\n",
      "Epoch 1, Batch 250/812, Loss: 2.1121, Training Accuracy: 27.31%\n",
      "Epoch 1, Batch 260/812, Loss: 1.9641, Training Accuracy: 28.11%\n",
      "Epoch 1, Batch 270/812, Loss: 2.0914, Training Accuracy: 28.80%\n",
      "Epoch 1, Batch 280/812, Loss: 1.8049, Training Accuracy: 29.64%\n",
      "Epoch 1, Batch 290/812, Loss: 1.8796, Training Accuracy: 30.32%\n",
      "Epoch 1, Batch 300/812, Loss: 1.7441, Training Accuracy: 31.16%\n",
      "Epoch 1, Batch 310/812, Loss: 1.9670, Training Accuracy: 31.74%\n",
      "Epoch 1, Batch 320/812, Loss: 1.8096, Training Accuracy: 32.42%\n",
      "Epoch 1, Batch 330/812, Loss: 1.7920, Training Accuracy: 32.99%\n",
      "Epoch 1, Batch 340/812, Loss: 1.6828, Training Accuracy: 33.69%\n",
      "Epoch 1, Batch 350/812, Loss: 1.8003, Training Accuracy: 34.29%\n",
      "Epoch 1, Batch 360/812, Loss: 1.6977, Training Accuracy: 34.92%\n",
      "Epoch 1, Batch 370/812, Loss: 1.6689, Training Accuracy: 35.46%\n",
      "Epoch 1, Batch 380/812, Loss: 1.7068, Training Accuracy: 36.00%\n",
      "Epoch 1, Batch 390/812, Loss: 1.5984, Training Accuracy: 36.57%\n",
      "Epoch 1, Batch 400/812, Loss: 1.5603, Training Accuracy: 37.15%\n",
      "Epoch 1, Batch 410/812, Loss: 1.5656, Training Accuracy: 37.69%\n",
      "Epoch 1, Batch 420/812, Loss: 1.5713, Training Accuracy: 38.22%\n",
      "Epoch 1, Batch 430/812, Loss: 1.6068, Training Accuracy: 38.68%\n",
      "Epoch 1, Batch 440/812, Loss: 1.5522, Training Accuracy: 39.18%\n",
      "Epoch 1, Batch 450/812, Loss: 1.6101, Training Accuracy: 39.61%\n",
      "Epoch 1, Batch 460/812, Loss: 1.6156, Training Accuracy: 40.00%\n",
      "Epoch 1, Batch 470/812, Loss: 1.5332, Training Accuracy: 40.43%\n",
      "Epoch 1, Batch 480/812, Loss: 1.4811, Training Accuracy: 40.84%\n",
      "Epoch 1, Batch 490/812, Loss: 1.5465, Training Accuracy: 41.19%\n",
      "Epoch 1, Batch 500/812, Loss: 1.5997, Training Accuracy: 41.54%\n",
      "Epoch 1, Batch 510/812, Loss: 1.4416, Training Accuracy: 41.94%\n",
      "Epoch 1, Batch 520/812, Loss: 1.4921, Training Accuracy: 42.27%\n",
      "Epoch 1, Batch 530/812, Loss: 1.3574, Training Accuracy: 42.69%\n",
      "Epoch 1, Batch 540/812, Loss: 1.3003, Training Accuracy: 43.11%\n",
      "Epoch 1, Batch 550/812, Loss: 1.4236, Training Accuracy: 43.45%\n",
      "Epoch 1, Batch 560/812, Loss: 1.3489, Training Accuracy: 43.82%\n",
      "Epoch 1, Batch 570/812, Loss: 1.3270, Training Accuracy: 44.21%\n",
      "Epoch 1, Batch 580/812, Loss: 1.4471, Training Accuracy: 44.48%\n",
      "Epoch 1, Batch 590/812, Loss: 1.3100, Training Accuracy: 44.82%\n",
      "Epoch 1, Batch 600/812, Loss: 1.3777, Training Accuracy: 45.13%\n",
      "Epoch 1, Batch 610/812, Loss: 1.4246, Training Accuracy: 45.39%\n",
      "Epoch 1, Batch 620/812, Loss: 1.3624, Training Accuracy: 45.66%\n",
      "Epoch 1, Batch 630/812, Loss: 1.3437, Training Accuracy: 45.95%\n",
      "Epoch 1, Batch 640/812, Loss: 1.2351, Training Accuracy: 46.28%\n",
      "Epoch 1, Batch 650/812, Loss: 1.2056, Training Accuracy: 46.59%\n",
      "Epoch 1, Batch 660/812, Loss: 1.2554, Training Accuracy: 46.89%\n",
      "Epoch 1, Batch 670/812, Loss: 1.1174, Training Accuracy: 47.21%\n",
      "Epoch 1, Batch 680/812, Loss: 1.2725, Training Accuracy: 47.50%\n",
      "Epoch 1, Batch 690/812, Loss: 1.1952, Training Accuracy: 47.82%\n",
      "Epoch 1, Batch 700/812, Loss: 1.1337, Training Accuracy: 48.14%\n",
      "Epoch 1, Batch 710/812, Loss: 1.1438, Training Accuracy: 48.43%\n",
      "Epoch 1, Batch 720/812, Loss: 1.1927, Training Accuracy: 48.68%\n",
      "Epoch 1, Batch 730/812, Loss: 1.1475, Training Accuracy: 48.95%\n",
      "Epoch 1, Batch 740/812, Loss: 1.1621, Training Accuracy: 49.22%\n",
      "Epoch 1, Batch 750/812, Loss: 1.1560, Training Accuracy: 49.48%\n",
      "Epoch 1, Batch 760/812, Loss: 1.2114, Training Accuracy: 49.73%\n",
      "Epoch 1, Batch 770/812, Loss: 1.1290, Training Accuracy: 49.99%\n",
      "Epoch 1, Batch 780/812, Loss: 1.0603, Training Accuracy: 50.24%\n",
      "Epoch 1, Batch 790/812, Loss: 1.1596, Training Accuracy: 50.47%\n",
      "Epoch 1, Batch 800/812, Loss: 1.1494, Training Accuracy: 50.72%\n",
      "Epoch 1, Batch 810/812, Loss: 1.0444, Training Accuracy: 50.97%\n",
      "Epoch 1, Training Accuracy: 51.01%, Validation Accuracy: 75.04%\n",
      "Epoch 2, Batch 10/812, Loss: 0.8935, Training Accuracy: 76.72%\n",
      "Epoch 2, Batch 20/812, Loss: 0.8229, Training Accuracy: 76.72%\n",
      "Epoch 2, Batch 30/812, Loss: 0.8789, Training Accuracy: 77.03%\n",
      "Epoch 2, Batch 40/812, Loss: 0.8308, Training Accuracy: 76.52%\n",
      "Epoch 2, Batch 50/812, Loss: 0.7781, Training Accuracy: 77.09%\n",
      "Epoch 2, Batch 60/812, Loss: 0.8025, Training Accuracy: 77.50%\n",
      "Epoch 2, Batch 70/812, Loss: 0.7989, Training Accuracy: 77.83%\n",
      "Epoch 2, Batch 80/812, Loss: 0.7317, Training Accuracy: 78.05%\n",
      "Epoch 2, Batch 90/812, Loss: 0.7840, Training Accuracy: 78.04%\n",
      "Epoch 2, Batch 100/812, Loss: 0.7588, Training Accuracy: 78.30%\n",
      "Epoch 2, Batch 110/812, Loss: 0.7904, Training Accuracy: 78.34%\n",
      "Epoch 2, Batch 120/812, Loss: 0.7423, Training Accuracy: 78.46%\n",
      "Epoch 2, Batch 130/812, Loss: 0.7487, Training Accuracy: 78.59%\n",
      "Epoch 2, Batch 140/812, Loss: 0.7607, Training Accuracy: 78.63%\n",
      "Epoch 2, Batch 150/812, Loss: 0.8148, Training Accuracy: 78.59%\n",
      "Epoch 2, Batch 160/812, Loss: 0.7583, Training Accuracy: 78.62%\n",
      "Epoch 2, Batch 170/812, Loss: 0.7939, Training Accuracy: 78.66%\n",
      "Epoch 2, Batch 180/812, Loss: 0.7234, Training Accuracy: 78.71%\n",
      "Epoch 2, Batch 190/812, Loss: 0.8556, Training Accuracy: 78.56%\n",
      "Epoch 2, Batch 200/812, Loss: 0.6718, Training Accuracy: 78.73%\n",
      "Epoch 2, Batch 210/812, Loss: 0.7095, Training Accuracy: 78.79%\n",
      "Epoch 2, Batch 220/812, Loss: 0.6897, Training Accuracy: 78.90%\n",
      "Epoch 2, Batch 230/812, Loss: 0.7495, Training Accuracy: 79.02%\n",
      "Epoch 2, Batch 240/812, Loss: 0.7067, Training Accuracy: 79.07%\n",
      "Epoch 2, Batch 250/812, Loss: 0.6908, Training Accuracy: 79.16%\n",
      "Epoch 2, Batch 260/812, Loss: 0.7314, Training Accuracy: 79.16%\n",
      "Epoch 2, Batch 270/812, Loss: 0.6728, Training Accuracy: 79.31%\n",
      "Epoch 2, Batch 280/812, Loss: 0.7697, Training Accuracy: 79.30%\n",
      "Epoch 2, Batch 290/812, Loss: 0.7335, Training Accuracy: 79.30%\n",
      "Epoch 2, Batch 300/812, Loss: 0.6522, Training Accuracy: 79.40%\n",
      "Epoch 2, Batch 310/812, Loss: 0.7230, Training Accuracy: 79.37%\n",
      "Epoch 2, Batch 320/812, Loss: 0.7201, Training Accuracy: 79.40%\n",
      "Epoch 2, Batch 330/812, Loss: 0.7269, Training Accuracy: 79.42%\n",
      "Epoch 2, Batch 340/812, Loss: 0.7101, Training Accuracy: 79.42%\n",
      "Epoch 2, Batch 350/812, Loss: 0.6970, Training Accuracy: 79.49%\n",
      "Epoch 2, Batch 360/812, Loss: 0.6129, Training Accuracy: 79.57%\n",
      "Epoch 2, Batch 370/812, Loss: 0.7296, Training Accuracy: 79.58%\n",
      "Epoch 2, Batch 380/812, Loss: 0.7313, Training Accuracy: 79.61%\n",
      "Epoch 2, Batch 390/812, Loss: 0.7314, Training Accuracy: 79.60%\n",
      "Epoch 2, Batch 400/812, Loss: 0.7276, Training Accuracy: 79.60%\n",
      "Epoch 2, Batch 410/812, Loss: 0.6246, Training Accuracy: 79.70%\n",
      "Epoch 2, Batch 420/812, Loss: 0.6735, Training Accuracy: 79.75%\n",
      "Epoch 2, Batch 430/812, Loss: 0.6516, Training Accuracy: 79.82%\n",
      "Epoch 2, Batch 440/812, Loss: 0.6590, Training Accuracy: 79.83%\n",
      "Epoch 2, Batch 450/812, Loss: 0.6272, Training Accuracy: 79.89%\n",
      "Epoch 2, Batch 460/812, Loss: 0.7180, Training Accuracy: 79.91%\n",
      "Epoch 2, Batch 470/812, Loss: 0.6888, Training Accuracy: 79.93%\n",
      "Epoch 2, Batch 480/812, Loss: 0.6736, Training Accuracy: 79.93%\n",
      "Epoch 2, Batch 490/812, Loss: 0.6491, Training Accuracy: 79.96%\n",
      "Epoch 2, Batch 500/812, Loss: 0.5877, Training Accuracy: 80.04%\n",
      "Epoch 2, Batch 510/812, Loss: 0.6996, Training Accuracy: 80.04%\n",
      "Epoch 2, Batch 520/812, Loss: 0.6483, Training Accuracy: 80.10%\n",
      "Epoch 2, Batch 530/812, Loss: 0.6180, Training Accuracy: 80.14%\n",
      "Epoch 2, Batch 540/812, Loss: 0.6136, Training Accuracy: 80.20%\n",
      "Epoch 2, Batch 550/812, Loss: 0.6913, Training Accuracy: 80.20%\n",
      "Epoch 2, Batch 560/812, Loss: 0.5455, Training Accuracy: 80.31%\n",
      "Epoch 2, Batch 570/812, Loss: 0.6249, Training Accuracy: 80.32%\n",
      "Epoch 2, Batch 580/812, Loss: 0.7253, Training Accuracy: 80.33%\n",
      "Epoch 2, Batch 590/812, Loss: 0.7147, Training Accuracy: 80.32%\n",
      "Epoch 2, Batch 600/812, Loss: 0.6120, Training Accuracy: 80.37%\n",
      "Epoch 2, Batch 610/812, Loss: 0.6166, Training Accuracy: 80.39%\n",
      "Epoch 2, Batch 620/812, Loss: 0.6342, Training Accuracy: 80.43%\n",
      "Epoch 2, Batch 630/812, Loss: 0.5733, Training Accuracy: 80.49%\n",
      "Epoch 2, Batch 640/812, Loss: 0.7042, Training Accuracy: 80.49%\n",
      "Epoch 2, Batch 650/812, Loss: 0.5523, Training Accuracy: 80.54%\n",
      "Epoch 2, Batch 660/812, Loss: 0.6123, Training Accuracy: 80.60%\n",
      "Epoch 2, Batch 670/812, Loss: 0.6272, Training Accuracy: 80.64%\n",
      "Epoch 2, Batch 680/812, Loss: 0.6485, Training Accuracy: 80.67%\n",
      "Epoch 2, Batch 690/812, Loss: 0.5356, Training Accuracy: 80.73%\n",
      "Epoch 2, Batch 700/812, Loss: 0.6563, Training Accuracy: 80.77%\n",
      "Epoch 2, Batch 710/812, Loss: 0.5852, Training Accuracy: 80.82%\n",
      "Epoch 2, Batch 720/812, Loss: 0.6258, Training Accuracy: 80.85%\n",
      "Epoch 2, Batch 730/812, Loss: 0.5764, Training Accuracy: 80.89%\n",
      "Epoch 2, Batch 740/812, Loss: 0.6285, Training Accuracy: 80.92%\n",
      "Epoch 2, Batch 750/812, Loss: 0.6062, Training Accuracy: 80.97%\n",
      "Epoch 2, Batch 760/812, Loss: 0.5980, Training Accuracy: 80.99%\n",
      "Epoch 2, Batch 770/812, Loss: 0.6948, Training Accuracy: 80.97%\n",
      "Epoch 2, Batch 780/812, Loss: 0.6115, Training Accuracy: 80.98%\n",
      "Epoch 2, Batch 790/812, Loss: 0.5712, Training Accuracy: 81.03%\n",
      "Epoch 2, Batch 800/812, Loss: 0.5995, Training Accuracy: 81.07%\n",
      "Epoch 2, Batch 810/812, Loss: 0.5878, Training Accuracy: 81.11%\n",
      "Epoch 2, Training Accuracy: 81.13%, Validation Accuracy: 88.93%\n",
      "Epoch 3, Batch 10/812, Loss: 0.3858, Training Accuracy: 88.59%\n",
      "Epoch 3, Batch 20/812, Loss: 0.3681, Training Accuracy: 88.98%\n",
      "Epoch 3, Batch 30/812, Loss: 0.3941, Training Accuracy: 88.80%\n",
      "Epoch 3, Batch 40/812, Loss: 0.3661, Training Accuracy: 88.75%\n",
      "Epoch 3, Batch 50/812, Loss: 0.3423, Training Accuracy: 89.06%\n",
      "Epoch 3, Batch 60/812, Loss: 0.3302, Training Accuracy: 89.61%\n",
      "Epoch 3, Batch 70/812, Loss: 0.3900, Training Accuracy: 89.60%\n",
      "Epoch 3, Batch 80/812, Loss: 0.2992, Training Accuracy: 89.84%\n",
      "Epoch 3, Batch 90/812, Loss: 0.3145, Training Accuracy: 90.16%\n",
      "Epoch 3, Batch 100/812, Loss: 0.3214, Training Accuracy: 90.19%\n",
      "Epoch 3, Batch 110/812, Loss: 0.2577, Training Accuracy: 90.37%\n",
      "Epoch 3, Batch 120/812, Loss: 0.2840, Training Accuracy: 90.57%\n",
      "Epoch 3, Batch 130/812, Loss: 0.2961, Training Accuracy: 90.75%\n",
      "Epoch 3, Batch 140/812, Loss: 0.2722, Training Accuracy: 90.88%\n",
      "Epoch 3, Batch 150/812, Loss: 0.3050, Training Accuracy: 90.84%\n",
      "Epoch 3, Batch 160/812, Loss: 0.2268, Training Accuracy: 91.06%\n",
      "Epoch 3, Batch 170/812, Loss: 0.3173, Training Accuracy: 91.08%\n",
      "Epoch 3, Batch 180/812, Loss: 0.3210, Training Accuracy: 91.09%\n",
      "Epoch 3, Batch 190/812, Loss: 0.2949, Training Accuracy: 91.13%\n",
      "Epoch 3, Batch 200/812, Loss: 0.3989, Training Accuracy: 91.04%\n",
      "Epoch 3, Batch 210/812, Loss: 0.3311, Training Accuracy: 91.09%\n",
      "Epoch 3, Batch 220/812, Loss: 0.2939, Training Accuracy: 91.12%\n",
      "Epoch 3, Batch 230/812, Loss: 0.3174, Training Accuracy: 91.11%\n",
      "Epoch 3, Batch 240/812, Loss: 0.3208, Training Accuracy: 91.09%\n",
      "Epoch 3, Batch 250/812, Loss: 0.2891, Training Accuracy: 91.08%\n",
      "Epoch 3, Batch 260/812, Loss: 0.3301, Training Accuracy: 91.08%\n",
      "Epoch 3, Batch 270/812, Loss: 0.2716, Training Accuracy: 91.11%\n",
      "Epoch 3, Batch 280/812, Loss: 0.2956, Training Accuracy: 91.14%\n",
      "Epoch 3, Batch 290/812, Loss: 0.2677, Training Accuracy: 91.20%\n",
      "Epoch 3, Batch 300/812, Loss: 0.2545, Training Accuracy: 91.23%\n",
      "Epoch 3, Batch 310/812, Loss: 0.2909, Training Accuracy: 91.24%\n",
      "Epoch 3, Batch 320/812, Loss: 0.2539, Training Accuracy: 91.30%\n",
      "Epoch 3, Batch 330/812, Loss: 0.2739, Training Accuracy: 91.34%\n",
      "Epoch 3, Batch 340/812, Loss: 0.2628, Training Accuracy: 91.36%\n",
      "Epoch 3, Batch 350/812, Loss: 0.3662, Training Accuracy: 91.33%\n",
      "Epoch 3, Batch 360/812, Loss: 0.3318, Training Accuracy: 91.27%\n",
      "Epoch 3, Batch 370/812, Loss: 0.3355, Training Accuracy: 91.27%\n",
      "Epoch 3, Batch 380/812, Loss: 0.3937, Training Accuracy: 91.23%\n",
      "Epoch 3, Batch 390/812, Loss: 0.2606, Training Accuracy: 91.28%\n",
      "Epoch 3, Batch 400/812, Loss: 0.2713, Training Accuracy: 91.29%\n",
      "Epoch 3, Batch 410/812, Loss: 0.2458, Training Accuracy: 91.33%\n",
      "Epoch 3, Batch 420/812, Loss: 0.3313, Training Accuracy: 91.31%\n",
      "Epoch 3, Batch 430/812, Loss: 0.2949, Training Accuracy: 91.33%\n",
      "Epoch 3, Batch 440/812, Loss: 0.3133, Training Accuracy: 91.34%\n",
      "Epoch 3, Batch 450/812, Loss: 0.3130, Training Accuracy: 91.29%\n",
      "Epoch 3, Batch 460/812, Loss: 0.2844, Training Accuracy: 91.31%\n",
      "Epoch 3, Batch 470/812, Loss: 0.3052, Training Accuracy: 91.32%\n",
      "Epoch 3, Batch 480/812, Loss: 0.3164, Training Accuracy: 91.32%\n",
      "Epoch 3, Batch 490/812, Loss: 0.2364, Training Accuracy: 91.36%\n",
      "Epoch 3, Batch 500/812, Loss: 0.2921, Training Accuracy: 91.37%\n",
      "Epoch 3, Batch 510/812, Loss: 0.3318, Training Accuracy: 91.35%\n",
      "Epoch 3, Batch 520/812, Loss: 0.3442, Training Accuracy: 91.35%\n",
      "Epoch 3, Batch 530/812, Loss: 0.3336, Training Accuracy: 91.34%\n",
      "Epoch 3, Batch 540/812, Loss: 0.3573, Training Accuracy: 91.32%\n",
      "Epoch 3, Batch 550/812, Loss: 0.2865, Training Accuracy: 91.31%\n",
      "Epoch 3, Batch 560/812, Loss: 0.2231, Training Accuracy: 91.35%\n",
      "Epoch 3, Batch 570/812, Loss: 0.3749, Training Accuracy: 91.32%\n",
      "Epoch 3, Batch 580/812, Loss: 0.3163, Training Accuracy: 91.31%\n",
      "Epoch 3, Batch 590/812, Loss: 0.3393, Training Accuracy: 91.30%\n",
      "Epoch 3, Batch 600/812, Loss: 0.3096, Training Accuracy: 91.29%\n",
      "Epoch 3, Batch 610/812, Loss: 0.2874, Training Accuracy: 91.31%\n",
      "Epoch 3, Batch 620/812, Loss: 0.2923, Training Accuracy: 91.32%\n",
      "Epoch 3, Batch 630/812, Loss: 0.2947, Training Accuracy: 91.32%\n",
      "Epoch 3, Batch 640/812, Loss: 0.2469, Training Accuracy: 91.34%\n",
      "Epoch 3, Batch 650/812, Loss: 0.2953, Training Accuracy: 91.33%\n",
      "Epoch 3, Batch 660/812, Loss: 0.3322, Training Accuracy: 91.30%\n",
      "Epoch 3, Batch 670/812, Loss: 0.3172, Training Accuracy: 91.29%\n",
      "Epoch 3, Batch 680/812, Loss: 0.2758, Training Accuracy: 91.31%\n",
      "Epoch 3, Batch 690/812, Loss: 0.2590, Training Accuracy: 91.31%\n",
      "Epoch 3, Batch 700/812, Loss: 0.3449, Training Accuracy: 91.30%\n",
      "Epoch 3, Batch 710/812, Loss: 0.3135, Training Accuracy: 91.29%\n",
      "Epoch 3, Batch 720/812, Loss: 0.3202, Training Accuracy: 91.28%\n",
      "Epoch 3, Batch 730/812, Loss: 0.2949, Training Accuracy: 91.27%\n",
      "Epoch 3, Batch 740/812, Loss: 0.3163, Training Accuracy: 91.25%\n",
      "Epoch 3, Batch 750/812, Loss: 0.3320, Training Accuracy: 91.23%\n",
      "Epoch 3, Batch 760/812, Loss: 0.2836, Training Accuracy: 91.24%\n",
      "Epoch 3, Batch 770/812, Loss: 0.2989, Training Accuracy: 91.24%\n",
      "Epoch 3, Batch 780/812, Loss: 0.3154, Training Accuracy: 91.23%\n",
      "Epoch 3, Batch 790/812, Loss: 0.2859, Training Accuracy: 91.23%\n",
      "Epoch 3, Batch 800/812, Loss: 0.3032, Training Accuracy: 91.22%\n",
      "Epoch 3, Batch 810/812, Loss: 0.2772, Training Accuracy: 91.24%\n",
      "Epoch 3, Training Accuracy: 91.24%, Validation Accuracy: 95.94%\n",
      "Epoch 4, Batch 10/812, Loss: 0.2227, Training Accuracy: 93.75%\n",
      "Epoch 4, Batch 20/812, Loss: 0.1308, Training Accuracy: 95.16%\n",
      "Epoch 4, Batch 30/812, Loss: 0.1348, Training Accuracy: 95.68%\n",
      "Epoch 4, Batch 40/812, Loss: 0.1153, Training Accuracy: 96.13%\n",
      "Epoch 4, Batch 50/812, Loss: 0.1266, Training Accuracy: 96.38%\n",
      "Epoch 4, Batch 60/812, Loss: 0.1311, Training Accuracy: 96.41%\n",
      "Epoch 4, Batch 70/812, Loss: 0.1072, Training Accuracy: 96.56%\n",
      "Epoch 4, Batch 80/812, Loss: 0.1254, Training Accuracy: 96.58%\n",
      "Epoch 4, Batch 90/812, Loss: 0.1307, Training Accuracy: 96.60%\n",
      "Epoch 4, Batch 100/812, Loss: 0.1352, Training Accuracy: 96.64%\n",
      "Epoch 4, Batch 110/812, Loss: 0.1118, Training Accuracy: 96.69%\n",
      "Epoch 4, Batch 120/812, Loss: 0.1627, Training Accuracy: 96.68%\n",
      "Epoch 4, Batch 130/812, Loss: 0.1109, Training Accuracy: 96.71%\n",
      "Epoch 4, Batch 140/812, Loss: 0.1110, Training Accuracy: 96.74%\n",
      "Epoch 4, Batch 150/812, Loss: 0.0950, Training Accuracy: 96.79%\n",
      "Epoch 4, Batch 160/812, Loss: 0.1082, Training Accuracy: 96.82%\n",
      "Epoch 4, Batch 170/812, Loss: 0.1414, Training Accuracy: 96.76%\n",
      "Epoch 4, Batch 180/812, Loss: 0.1006, Training Accuracy: 96.79%\n",
      "Epoch 4, Batch 190/812, Loss: 0.1386, Training Accuracy: 96.74%\n",
      "Epoch 4, Batch 200/812, Loss: 0.1127, Training Accuracy: 96.77%\n",
      "Epoch 4, Batch 210/812, Loss: 0.1082, Training Accuracy: 96.78%\n",
      "Epoch 4, Batch 220/812, Loss: 0.1012, Training Accuracy: 96.79%\n",
      "Epoch 4, Batch 230/812, Loss: 0.1084, Training Accuracy: 96.82%\n",
      "Epoch 4, Batch 240/812, Loss: 0.0914, Training Accuracy: 96.88%\n",
      "Epoch 4, Batch 250/812, Loss: 0.1074, Training Accuracy: 96.90%\n",
      "Epoch 4, Batch 260/812, Loss: 0.1250, Training Accuracy: 96.84%\n",
      "Epoch 4, Batch 270/812, Loss: 0.1153, Training Accuracy: 96.85%\n",
      "Epoch 4, Batch 280/812, Loss: 0.0980, Training Accuracy: 96.88%\n",
      "Epoch 4, Batch 290/812, Loss: 0.1333, Training Accuracy: 96.87%\n",
      "Epoch 4, Batch 300/812, Loss: 0.1108, Training Accuracy: 96.88%\n",
      "Epoch 4, Batch 310/812, Loss: 0.0841, Training Accuracy: 96.93%\n",
      "Epoch 4, Batch 320/812, Loss: 0.1476, Training Accuracy: 96.89%\n",
      "Epoch 4, Batch 330/812, Loss: 0.1022, Training Accuracy: 96.88%\n",
      "Epoch 4, Batch 340/812, Loss: 0.1318, Training Accuracy: 96.85%\n",
      "Epoch 4, Batch 350/812, Loss: 0.1006, Training Accuracy: 96.86%\n",
      "Epoch 4, Batch 360/812, Loss: 0.1365, Training Accuracy: 96.85%\n",
      "Epoch 4, Batch 370/812, Loss: 0.0945, Training Accuracy: 96.86%\n",
      "Epoch 4, Batch 380/812, Loss: 0.1054, Training Accuracy: 96.87%\n",
      "Epoch 4, Batch 390/812, Loss: 0.1520, Training Accuracy: 96.85%\n",
      "Epoch 4, Batch 400/812, Loss: 0.1264, Training Accuracy: 96.84%\n",
      "Epoch 4, Batch 410/812, Loss: 0.0995, Training Accuracy: 96.84%\n",
      "Epoch 4, Batch 420/812, Loss: 0.1169, Training Accuracy: 96.85%\n",
      "Epoch 4, Batch 430/812, Loss: 0.1014, Training Accuracy: 96.86%\n",
      "Epoch 4, Batch 440/812, Loss: 0.1368, Training Accuracy: 96.84%\n",
      "Epoch 4, Batch 450/812, Loss: 0.1344, Training Accuracy: 96.83%\n",
      "Epoch 4, Batch 460/812, Loss: 0.0895, Training Accuracy: 96.85%\n",
      "Epoch 4, Batch 470/812, Loss: 0.1445, Training Accuracy: 96.85%\n",
      "Epoch 4, Batch 480/812, Loss: 0.1547, Training Accuracy: 96.82%\n",
      "Epoch 4, Batch 490/812, Loss: 0.1425, Training Accuracy: 96.80%\n",
      "Epoch 4, Batch 500/812, Loss: 0.1273, Training Accuracy: 96.79%\n",
      "Epoch 4, Batch 510/812, Loss: 0.1259, Training Accuracy: 96.79%\n",
      "Epoch 4, Batch 520/812, Loss: 0.1309, Training Accuracy: 96.76%\n",
      "Epoch 4, Batch 530/812, Loss: 0.1235, Training Accuracy: 96.77%\n",
      "Epoch 4, Batch 540/812, Loss: 0.1255, Training Accuracy: 96.77%\n",
      "Epoch 4, Batch 550/812, Loss: 0.1278, Training Accuracy: 96.76%\n",
      "Epoch 4, Batch 560/812, Loss: 0.1111, Training Accuracy: 96.77%\n",
      "Epoch 4, Batch 570/812, Loss: 0.1087, Training Accuracy: 96.77%\n",
      "Epoch 4, Batch 580/812, Loss: 0.1340, Training Accuracy: 96.76%\n",
      "Epoch 4, Batch 590/812, Loss: 0.1394, Training Accuracy: 96.75%\n",
      "Epoch 4, Batch 600/812, Loss: 0.1358, Training Accuracy: 96.73%\n",
      "Epoch 4, Batch 610/812, Loss: 0.1458, Training Accuracy: 96.72%\n",
      "Epoch 4, Batch 620/812, Loss: 0.1239, Training Accuracy: 96.71%\n",
      "Epoch 4, Batch 630/812, Loss: 0.0971, Training Accuracy: 96.72%\n",
      "Epoch 4, Batch 640/812, Loss: 0.1834, Training Accuracy: 96.69%\n",
      "Epoch 4, Batch 650/812, Loss: 0.1433, Training Accuracy: 96.68%\n",
      "Epoch 4, Batch 660/812, Loss: 0.1108, Training Accuracy: 96.68%\n",
      "Epoch 4, Batch 670/812, Loss: 0.1201, Training Accuracy: 96.69%\n",
      "Epoch 4, Batch 680/812, Loss: 0.1228, Training Accuracy: 96.68%\n",
      "Epoch 4, Batch 690/812, Loss: 0.1556, Training Accuracy: 96.68%\n",
      "Epoch 4, Batch 700/812, Loss: 0.1302, Training Accuracy: 96.67%\n",
      "Epoch 4, Batch 710/812, Loss: 0.1618, Training Accuracy: 96.64%\n",
      "Epoch 4, Batch 720/812, Loss: 0.1395, Training Accuracy: 96.63%\n",
      "Epoch 4, Batch 730/812, Loss: 0.1596, Training Accuracy: 96.61%\n",
      "Epoch 4, Batch 740/812, Loss: 0.1163, Training Accuracy: 96.62%\n",
      "Epoch 4, Batch 750/812, Loss: 0.1059, Training Accuracy: 96.62%\n",
      "Epoch 4, Batch 760/812, Loss: 0.1103, Training Accuracy: 96.62%\n",
      "Epoch 4, Batch 770/812, Loss: 0.1159, Training Accuracy: 96.62%\n",
      "Epoch 4, Batch 780/812, Loss: 0.1273, Training Accuracy: 96.61%\n",
      "Epoch 4, Batch 790/812, Loss: 0.1062, Training Accuracy: 96.62%\n",
      "Epoch 4, Batch 800/812, Loss: 0.1405, Training Accuracy: 96.61%\n",
      "Epoch 4, Batch 810/812, Loss: 0.1134, Training Accuracy: 96.60%\n",
      "Epoch 4, Training Accuracy: 96.61%, Validation Accuracy: 98.74%\n",
      "Epoch 5, Batch 10/812, Loss: 0.0559, Training Accuracy: 98.75%\n",
      "Epoch 5, Batch 20/812, Loss: 0.0538, Training Accuracy: 98.91%\n",
      "Epoch 5, Batch 30/812, Loss: 0.0515, Training Accuracy: 98.96%\n",
      "Epoch 5, Batch 40/812, Loss: 0.0527, Training Accuracy: 98.95%\n",
      "Epoch 5, Batch 50/812, Loss: 0.0459, Training Accuracy: 98.91%\n",
      "Epoch 5, Batch 60/812, Loss: 0.0293, Training Accuracy: 98.98%\n",
      "Epoch 5, Batch 70/812, Loss: 0.0565, Training Accuracy: 99.00%\n",
      "Epoch 5, Batch 80/812, Loss: 0.0294, Training Accuracy: 99.10%\n",
      "Epoch 5, Batch 90/812, Loss: 0.0354, Training Accuracy: 99.13%\n",
      "Epoch 5, Batch 100/812, Loss: 0.0512, Training Accuracy: 99.09%\n",
      "Epoch 5, Batch 110/812, Loss: 0.0433, Training Accuracy: 99.09%\n",
      "Epoch 5, Batch 120/812, Loss: 0.0460, Training Accuracy: 99.08%\n",
      "Epoch 5, Batch 130/812, Loss: 0.0353, Training Accuracy: 99.07%\n",
      "Epoch 5, Batch 140/812, Loss: 0.0318, Training Accuracy: 99.12%\n",
      "Epoch 5, Batch 150/812, Loss: 0.0271, Training Accuracy: 99.15%\n",
      "Epoch 5, Batch 160/812, Loss: 0.0476, Training Accuracy: 99.09%\n",
      "Epoch 5, Batch 170/812, Loss: 0.0409, Training Accuracy: 99.11%\n",
      "Epoch 5, Batch 180/812, Loss: 0.0518, Training Accuracy: 99.09%\n",
      "Epoch 5, Batch 190/812, Loss: 0.0359, Training Accuracy: 99.10%\n",
      "Epoch 5, Batch 200/812, Loss: 0.0375, Training Accuracy: 99.12%\n",
      "Epoch 5, Batch 210/812, Loss: 0.0366, Training Accuracy: 99.13%\n",
      "Epoch 5, Batch 220/812, Loss: 0.0363, Training Accuracy: 99.14%\n",
      "Epoch 5, Batch 230/812, Loss: 0.0413, Training Accuracy: 99.14%\n",
      "Epoch 5, Batch 240/812, Loss: 0.0290, Training Accuracy: 99.16%\n",
      "Epoch 5, Batch 250/812, Loss: 0.0376, Training Accuracy: 99.16%\n",
      "Epoch 5, Batch 260/812, Loss: 0.0362, Training Accuracy: 99.17%\n",
      "Epoch 5, Batch 270/812, Loss: 0.0284, Training Accuracy: 99.19%\n",
      "Epoch 5, Batch 280/812, Loss: 0.0378, Training Accuracy: 99.19%\n",
      "Epoch 5, Batch 290/812, Loss: 0.0377, Training Accuracy: 99.19%\n",
      "Epoch 5, Batch 300/812, Loss: 0.0244, Training Accuracy: 99.19%\n",
      "Epoch 5, Batch 310/812, Loss: 0.0321, Training Accuracy: 99.19%\n",
      "Epoch 5, Batch 320/812, Loss: 0.0367, Training Accuracy: 99.21%\n",
      "Epoch 5, Batch 330/812, Loss: 0.0329, Training Accuracy: 99.21%\n",
      "Epoch 5, Batch 340/812, Loss: 0.0329, Training Accuracy: 99.22%\n",
      "Epoch 5, Batch 350/812, Loss: 0.0362, Training Accuracy: 99.22%\n",
      "Epoch 5, Batch 360/812, Loss: 0.0265, Training Accuracy: 99.22%\n",
      "Epoch 5, Batch 370/812, Loss: 0.0272, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 380/812, Loss: 0.0281, Training Accuracy: 99.24%\n",
      "Epoch 5, Batch 390/812, Loss: 0.0431, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 400/812, Loss: 0.0276, Training Accuracy: 99.24%\n",
      "Epoch 5, Batch 410/812, Loss: 0.0324, Training Accuracy: 99.25%\n",
      "Epoch 5, Batch 420/812, Loss: 0.0234, Training Accuracy: 99.25%\n",
      "Epoch 5, Batch 430/812, Loss: 0.0316, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 440/812, Loss: 0.0219, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 450/812, Loss: 0.0251, Training Accuracy: 99.28%\n",
      "Epoch 5, Batch 460/812, Loss: 0.0236, Training Accuracy: 99.29%\n",
      "Epoch 5, Batch 470/812, Loss: 0.0457, Training Accuracy: 99.30%\n",
      "Epoch 5, Batch 480/812, Loss: 0.0378, Training Accuracy: 99.29%\n",
      "Epoch 5, Batch 490/812, Loss: 0.0244, Training Accuracy: 99.30%\n",
      "Epoch 5, Batch 500/812, Loss: 0.0307, Training Accuracy: 99.30%\n",
      "Epoch 5, Batch 510/812, Loss: 0.0459, Training Accuracy: 99.30%\n",
      "Epoch 5, Batch 520/812, Loss: 0.0376, Training Accuracy: 99.29%\n",
      "Epoch 5, Batch 530/812, Loss: 0.0365, Training Accuracy: 99.29%\n",
      "Epoch 5, Batch 540/812, Loss: 0.0373, Training Accuracy: 99.29%\n",
      "Epoch 5, Batch 550/812, Loss: 0.0335, Training Accuracy: 99.29%\n",
      "Epoch 5, Batch 560/812, Loss: 0.0445, Training Accuracy: 99.28%\n",
      "Epoch 5, Batch 570/812, Loss: 0.0334, Training Accuracy: 99.28%\n",
      "Epoch 5, Batch 580/812, Loss: 0.0311, Training Accuracy: 99.28%\n",
      "Epoch 5, Batch 590/812, Loss: 0.0413, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 600/812, Loss: 0.0285, Training Accuracy: 99.28%\n",
      "Epoch 5, Batch 610/812, Loss: 0.0293, Training Accuracy: 99.28%\n",
      "Epoch 5, Batch 620/812, Loss: 0.0364, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 630/812, Loss: 0.0402, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 640/812, Loss: 0.0232, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 650/812, Loss: 0.0386, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 660/812, Loss: 0.0519, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 670/812, Loss: 0.0439, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 680/812, Loss: 0.0259, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 690/812, Loss: 0.0385, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 700/812, Loss: 0.0306, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 710/812, Loss: 0.0341, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 720/812, Loss: 0.0296, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 730/812, Loss: 0.0427, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 740/812, Loss: 0.0465, Training Accuracy: 99.25%\n",
      "Epoch 5, Batch 750/812, Loss: 0.0497, Training Accuracy: 99.24%\n",
      "Epoch 5, Batch 760/812, Loss: 0.0343, Training Accuracy: 99.24%\n",
      "Epoch 5, Batch 770/812, Loss: 0.0407, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 780/812, Loss: 0.0395, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 790/812, Loss: 0.0412, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 800/812, Loss: 0.0264, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 810/812, Loss: 0.0337, Training Accuracy: 99.24%\n",
      "Epoch 5, Training Accuracy: 99.24%, Validation Accuracy: 99.76%\n",
      "Epoch 6, Batch 10/812, Loss: 0.0255, Training Accuracy: 99.69%\n",
      "Epoch 6, Batch 20/812, Loss: 0.0128, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 30/812, Loss: 0.0128, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 40/812, Loss: 0.0147, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 50/812, Loss: 0.0123, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 60/812, Loss: 0.0114, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 70/812, Loss: 0.0145, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 80/812, Loss: 0.0179, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 90/812, Loss: 0.0100, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 100/812, Loss: 0.0152, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 110/812, Loss: 0.0103, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 120/812, Loss: 0.0122, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 130/812, Loss: 0.0112, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 140/812, Loss: 0.0130, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 150/812, Loss: 0.0104, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 160/812, Loss: 0.0108, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 170/812, Loss: 0.0072, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 180/812, Loss: 0.0158, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 190/812, Loss: 0.0094, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 200/812, Loss: 0.0166, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 210/812, Loss: 0.0109, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 220/812, Loss: 0.0090, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 230/812, Loss: 0.0123, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 240/812, Loss: 0.0078, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 250/812, Loss: 0.0099, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 260/812, Loss: 0.0084, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 270/812, Loss: 0.0076, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 280/812, Loss: 0.0068, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 290/812, Loss: 0.0090, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 300/812, Loss: 0.0140, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 310/812, Loss: 0.0060, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 320/812, Loss: 0.0094, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 330/812, Loss: 0.0245, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 340/812, Loss: 0.0071, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 350/812, Loss: 0.0135, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 360/812, Loss: 0.0079, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 370/812, Loss: 0.0080, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 380/812, Loss: 0.0113, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 390/812, Loss: 0.0073, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 400/812, Loss: 0.0096, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 410/812, Loss: 0.0124, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 420/812, Loss: 0.0097, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 430/812, Loss: 0.0124, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 440/812, Loss: 0.0079, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 450/812, Loss: 0.0081, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 460/812, Loss: 0.0068, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 470/812, Loss: 0.0085, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 480/812, Loss: 0.0134, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 490/812, Loss: 0.0080, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 500/812, Loss: 0.0073, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 510/812, Loss: 0.0059, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 520/812, Loss: 0.0098, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 530/812, Loss: 0.0087, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 540/812, Loss: 0.0087, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 550/812, Loss: 0.0065, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 560/812, Loss: 0.0130, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 570/812, Loss: 0.0109, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 580/812, Loss: 0.0095, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 590/812, Loss: 0.0094, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 600/812, Loss: 0.0091, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 610/812, Loss: 0.0067, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 620/812, Loss: 0.0135, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 630/812, Loss: 0.0126, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 640/812, Loss: 0.0089, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 650/812, Loss: 0.0057, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 660/812, Loss: 0.0125, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 670/812, Loss: 0.0088, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 680/812, Loss: 0.0087, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 690/812, Loss: 0.0089, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 700/812, Loss: 0.0174, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 710/812, Loss: 0.0107, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 720/812, Loss: 0.0130, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 730/812, Loss: 0.0173, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 740/812, Loss: 0.0079, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 750/812, Loss: 0.0104, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 760/812, Loss: 0.0075, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 770/812, Loss: 0.0115, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 780/812, Loss: 0.0071, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 790/812, Loss: 0.0075, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 800/812, Loss: 0.0127, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 810/812, Loss: 0.0100, Training Accuracy: 99.89%\n",
      "Epoch 6, Training Accuracy: 99.89%, Validation Accuracy: 99.94%\n",
      "Epoch 7, Batch 10/812, Loss: 0.0039, Training Accuracy: 100.00%\n",
      "Epoch 7, Batch 20/812, Loss: 0.0035, Training Accuracy: 100.00%\n",
      "Epoch 7, Batch 30/812, Loss: 0.0032, Training Accuracy: 100.00%\n",
      "Epoch 7, Batch 40/812, Loss: 0.0053, Training Accuracy: 100.00%\n",
      "Epoch 7, Batch 50/812, Loss: 0.0048, Training Accuracy: 100.00%\n",
      "Epoch 7, Batch 60/812, Loss: 0.0071, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 70/812, Loss: 0.0038, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 80/812, Loss: 0.0048, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 90/812, Loss: 0.0126, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 100/812, Loss: 0.0050, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 110/812, Loss: 0.0039, Training Accuracy: 99.96%\n",
      "Epoch 7, Batch 120/812, Loss: 0.0039, Training Accuracy: 99.96%\n",
      "Epoch 7, Batch 130/812, Loss: 0.0042, Training Accuracy: 99.96%\n",
      "Epoch 7, Batch 140/812, Loss: 0.0042, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 150/812, Loss: 0.0035, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 160/812, Loss: 0.0036, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 170/812, Loss: 0.0043, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 180/812, Loss: 0.0038, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 190/812, Loss: 0.0034, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 200/812, Loss: 0.0104, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 210/812, Loss: 0.0046, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 220/812, Loss: 0.0032, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 230/812, Loss: 0.0032, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 240/812, Loss: 0.0036, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 250/812, Loss: 0.0036, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 260/812, Loss: 0.0031, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 270/812, Loss: 0.0038, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 280/812, Loss: 0.0034, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 290/812, Loss: 0.0039, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 300/812, Loss: 0.0031, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 310/812, Loss: 0.0051, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 320/812, Loss: 0.0040, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 330/812, Loss: 0.0038, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 340/812, Loss: 0.0043, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 350/812, Loss: 0.0100, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 360/812, Loss: 0.0044, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 370/812, Loss: 0.0031, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 380/812, Loss: 0.0030, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 390/812, Loss: 0.0033, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 400/812, Loss: 0.0033, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 410/812, Loss: 0.0022, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 420/812, Loss: 0.0034, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 430/812, Loss: 0.0035, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 440/812, Loss: 0.0035, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 450/812, Loss: 0.0053, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 460/812, Loss: 0.0031, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 470/812, Loss: 0.0044, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 480/812, Loss: 0.0102, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 490/812, Loss: 0.0053, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 500/812, Loss: 0.0031, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 510/812, Loss: 0.0039, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 520/812, Loss: 0.0032, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 530/812, Loss: 0.0035, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 540/812, Loss: 0.0028, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 550/812, Loss: 0.0033, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 560/812, Loss: 0.0039, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 570/812, Loss: 0.0029, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 580/812, Loss: 0.0026, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 590/812, Loss: 0.0029, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 600/812, Loss: 0.0033, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 610/812, Loss: 0.0032, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 620/812, Loss: 0.0034, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 630/812, Loss: 0.0044, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 640/812, Loss: 0.0029, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 650/812, Loss: 0.0032, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 660/812, Loss: 0.0044, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 670/812, Loss: 0.0029, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 680/812, Loss: 0.0023, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 690/812, Loss: 0.0031, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 700/812, Loss: 0.0027, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 710/812, Loss: 0.0027, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 720/812, Loss: 0.0043, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 730/812, Loss: 0.0025, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 740/812, Loss: 0.0076, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 750/812, Loss: 0.0038, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 760/812, Loss: 0.0030, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 770/812, Loss: 0.0033, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 780/812, Loss: 0.0037, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 790/812, Loss: 0.0046, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 800/812, Loss: 0.0032, Training Accuracy: 99.98%\n",
      "Epoch 7, Batch 810/812, Loss: 0.0024, Training Accuracy: 99.98%\n",
      "Epoch 7, Training Accuracy: 99.98%, Validation Accuracy: 99.98%\n",
      "Epoch 8, Batch 10/812, Loss: 0.0021, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 20/812, Loss: 0.0020, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 30/812, Loss: 0.0023, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 40/812, Loss: 0.0021, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 50/812, Loss: 0.0020, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 60/812, Loss: 0.0026, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 70/812, Loss: 0.0025, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 80/812, Loss: 0.0023, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 90/812, Loss: 0.0025, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 100/812, Loss: 0.0025, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 110/812, Loss: 0.0021, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 120/812, Loss: 0.0023, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 130/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 140/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 150/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 160/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 170/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 180/812, Loss: 0.0018, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 190/812, Loss: 0.0023, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 200/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 210/812, Loss: 0.0020, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 220/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 230/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 240/812, Loss: 0.0018, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 250/812, Loss: 0.0020, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 260/812, Loss: 0.0023, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 270/812, Loss: 0.0017, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 280/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 290/812, Loss: 0.0023, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 300/812, Loss: 0.0018, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 310/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 320/812, Loss: 0.0020, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 330/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 340/812, Loss: 0.0020, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 350/812, Loss: 0.0021, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 360/812, Loss: 0.0020, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 370/812, Loss: 0.0034, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 380/812, Loss: 0.0019, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 390/812, Loss: 0.0054, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 400/812, Loss: 0.0024, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 410/812, Loss: 0.0025, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 420/812, Loss: 0.0027, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 430/812, Loss: 0.0019, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 440/812, Loss: 0.0023, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 450/812, Loss: 0.0024, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 460/812, Loss: 0.0021, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 470/812, Loss: 0.0020, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 480/812, Loss: 0.0028, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 490/812, Loss: 0.0020, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 500/812, Loss: 0.0029, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 510/812, Loss: 0.0031, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 520/812, Loss: 0.0025, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 530/812, Loss: 0.0020, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 540/812, Loss: 0.0021, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 550/812, Loss: 0.0020, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 560/812, Loss: 0.0016, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 570/812, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 580/812, Loss: 0.0021, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 590/812, Loss: 0.0018, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 600/812, Loss: 0.0019, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 610/812, Loss: 0.0019, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 620/812, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 630/812, Loss: 0.0021, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 640/812, Loss: 0.0018, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 650/812, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 660/812, Loss: 0.0016, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 670/812, Loss: 0.0018, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 680/812, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 690/812, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 700/812, Loss: 0.0019, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 710/812, Loss: 0.0018, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 720/812, Loss: 0.0019, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 730/812, Loss: 0.0020, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 740/812, Loss: 0.0024, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 750/812, Loss: 0.0016, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 760/812, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 770/812, Loss: 0.0015, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 780/812, Loss: 0.0019, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 790/812, Loss: 0.0018, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 800/812, Loss: 0.0019, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 810/812, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 8, Training Accuracy: 99.99%, Validation Accuracy: 100.00%\n",
      "Epoch 9, Batch 10/812, Loss: 0.0021, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 20/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 30/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 40/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 50/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 60/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 70/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 80/812, Loss: 0.0017, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 90/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 100/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 110/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 120/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 130/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 140/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 150/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 160/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 170/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 180/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 190/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 200/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 210/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 220/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 230/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 240/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 250/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 260/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 270/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 280/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 290/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 300/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 310/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 320/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 330/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 340/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 350/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 360/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 370/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 380/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 390/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 400/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 410/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 420/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 430/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 440/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 450/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 460/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 470/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 480/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 490/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 500/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 510/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 520/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 530/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 540/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 550/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 560/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 570/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 580/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 590/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 600/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 610/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 620/812, Loss: 0.0026, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 630/812, Loss: 0.0019, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 640/812, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 650/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 660/812, Loss: 0.0019, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 670/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 680/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 690/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 700/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 710/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 720/812, Loss: 0.0018, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 730/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 740/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 750/812, Loss: 0.0016, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 760/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 770/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 780/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 790/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 800/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 810/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 10, Batch 10/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 20/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 30/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 40/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 50/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 60/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 70/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 80/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 90/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 100/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 110/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 120/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 130/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 140/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 150/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 160/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 170/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 180/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 190/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 200/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 210/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 220/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 230/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 240/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 250/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 260/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 270/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 280/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 290/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 300/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 310/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 320/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 330/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 340/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 350/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 360/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 370/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 380/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 390/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 400/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 410/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 420/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 430/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 440/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 450/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 460/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 470/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 480/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 490/812, Loss: 0.0015, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 500/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 510/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 520/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 530/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 540/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 550/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 560/812, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 570/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 580/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 590/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 600/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 610/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 620/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 630/812, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 640/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 650/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 660/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 670/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 680/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 690/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 700/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 710/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 720/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 730/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 740/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 750/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 760/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 770/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 780/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 790/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 800/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 810/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 10, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 11, Batch 10/812, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 20/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 30/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 40/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 50/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 60/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 70/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 80/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 90/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 100/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 110/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 120/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 130/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 140/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 150/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 160/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 170/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 180/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 190/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 200/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 210/812, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 220/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 230/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 240/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 250/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 260/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 270/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 280/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 290/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 300/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 310/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 320/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 330/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 340/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 350/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 360/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 370/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 380/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 390/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 400/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 410/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 420/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 430/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 440/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 450/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 460/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 470/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 480/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 490/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 500/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 510/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 520/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 530/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 540/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 550/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 560/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 570/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 580/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 590/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 600/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 610/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 620/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 630/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 640/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 650/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 660/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 670/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 680/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 690/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 700/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 710/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 720/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 730/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 740/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 750/812, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 760/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 770/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 780/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 790/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 800/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 810/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 11, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 12, Batch 10/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 20/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 30/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 40/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 50/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 60/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 70/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 80/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 90/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 100/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 110/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 120/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 130/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 140/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 150/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 160/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 170/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 180/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 190/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 200/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 210/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 220/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 230/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 240/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 250/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 260/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 270/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 280/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 290/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 300/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 310/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 320/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 330/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 340/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 350/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 360/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 370/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 380/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 390/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 400/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 410/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 420/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 430/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 440/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 450/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 460/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 470/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 480/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 490/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 500/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 510/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 520/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 530/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 540/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 550/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 560/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 570/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 580/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 590/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 600/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 610/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 620/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 630/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 640/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 650/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 660/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 670/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 680/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 690/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 700/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 710/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 720/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 730/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 740/812, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 750/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 760/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 770/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 780/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 790/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 800/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 810/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 12, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 13, Batch 10/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 20/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 30/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 40/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 50/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 60/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 70/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 80/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 90/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 100/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 110/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 120/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 130/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 140/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 150/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 160/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 170/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 180/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 190/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 200/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 210/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 220/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 230/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 240/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 250/812, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 260/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 270/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 280/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 290/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 300/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 310/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 320/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 330/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 340/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 350/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 360/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 370/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 380/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 390/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 400/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 410/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 420/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 430/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 440/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 450/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 460/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 470/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 480/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 490/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 500/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 510/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 520/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 530/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 540/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 550/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 560/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 570/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 580/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 590/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 600/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 610/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 620/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 630/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 640/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 650/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 660/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 670/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 680/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 690/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 700/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 710/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 720/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 730/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 740/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 750/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 760/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 770/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 780/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 790/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 800/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 810/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 14, Batch 10/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 20/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 30/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 40/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 50/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 60/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 70/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 80/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 90/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 100/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 110/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 120/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 130/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 140/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 150/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 160/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 170/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 180/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 190/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 200/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 210/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 220/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 230/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 240/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 250/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 260/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 270/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 280/812, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 290/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 300/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 310/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 320/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 330/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 340/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 350/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 360/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 370/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 380/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 390/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 400/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 410/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 420/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 430/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 440/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 450/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 460/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 470/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 480/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 490/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 500/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 510/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 520/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 530/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 540/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 550/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 560/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 570/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 580/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 590/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 600/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 610/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 620/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 630/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 640/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 650/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 660/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 670/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 680/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 690/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 700/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 710/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 720/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 730/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 740/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 750/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 760/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 770/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 780/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 790/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 800/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 810/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 14, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 15, Batch 10/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 20/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 30/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 40/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 50/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 60/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 70/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 80/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 90/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 100/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 110/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 120/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 130/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 140/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 150/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 160/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 170/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 180/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 190/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 200/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 210/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 220/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 230/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 240/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 250/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 260/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 270/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 280/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 290/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 300/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 310/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 320/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 330/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 340/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 350/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 360/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 370/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 380/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 390/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 400/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 410/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 420/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 430/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 440/812, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 450/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 460/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 470/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 480/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 490/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 500/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 510/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 520/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 530/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 540/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 550/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 560/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 570/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 580/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 590/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 600/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 610/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 620/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 630/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 640/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 650/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 660/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 670/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 680/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 690/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 700/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 710/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 720/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 730/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 740/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 750/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 760/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 770/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 780/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 790/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 800/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 810/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 15, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 16, Batch 10/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 20/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 30/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 40/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 50/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 60/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 70/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 80/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 90/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 100/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 110/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 120/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 130/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 140/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 150/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 160/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 170/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 180/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 190/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 200/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 210/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 220/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 230/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 240/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 250/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 260/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 270/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 280/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 290/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 300/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 310/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 320/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 330/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 340/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 350/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 360/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 370/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 380/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 390/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 400/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 410/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 420/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 430/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 440/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 450/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 460/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 470/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 480/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 490/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 500/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 510/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 520/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 530/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 540/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 550/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 560/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 570/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 580/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 590/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 600/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 610/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 620/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 630/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 640/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 650/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 660/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 670/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 680/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 690/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 700/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 710/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 720/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 730/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 740/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 750/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 760/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 770/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 780/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 790/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 800/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 810/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 16, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 17, Batch 10/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 20/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 30/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 40/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 50/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 60/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 70/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 80/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 90/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 100/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 110/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 120/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 130/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 140/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 150/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 160/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 170/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 180/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 190/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 200/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 210/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 220/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 230/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 240/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 250/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 260/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 270/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 280/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 290/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 300/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 310/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 320/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 330/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 340/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 350/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 360/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 370/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 380/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 390/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 400/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 410/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 420/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 430/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 440/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 450/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 460/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 470/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 480/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 490/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 500/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 510/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 520/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 530/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 540/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 550/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 560/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 570/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 580/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 590/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 600/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 610/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 620/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 630/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 640/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 650/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 660/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 670/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 680/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 690/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 700/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 710/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 720/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 730/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 740/812, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 750/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 760/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 770/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 780/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 790/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 800/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 810/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 17, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 18, Batch 10/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 20/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 30/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 40/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 50/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 60/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 70/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 80/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 90/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 100/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 110/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 120/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 130/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 140/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 150/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 160/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 170/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 180/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 190/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 200/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 210/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 220/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 230/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 240/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 250/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 260/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 270/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 280/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 290/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 300/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 310/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 320/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 330/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 340/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 350/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 360/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 370/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 380/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 390/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 400/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 410/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 420/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 430/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 440/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 450/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 460/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 470/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 480/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 490/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 500/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 510/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 520/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 530/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 540/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 550/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 560/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 570/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 580/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 590/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 600/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 610/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 620/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 630/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 640/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 650/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 660/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 670/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 680/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 690/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 700/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 710/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 720/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 730/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 740/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 750/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 760/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 770/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 780/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 790/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 800/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 810/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 18, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 19, Batch 10/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 20/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 30/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 40/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 50/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 60/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 70/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 80/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 90/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 100/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 110/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 120/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 130/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 140/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 150/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 160/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 170/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 180/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 190/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 200/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 210/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 220/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 230/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 240/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 250/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 260/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 270/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 280/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 290/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 300/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 310/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 320/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 330/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 340/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 350/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 360/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 370/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 380/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 390/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 400/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 410/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 420/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 430/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 440/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 450/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 460/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 470/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 480/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 490/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 500/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 510/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 520/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 530/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 540/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 550/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 560/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 570/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 580/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 590/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 600/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 610/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 620/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 630/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 640/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 650/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 660/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 670/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 680/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 690/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 700/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 710/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 720/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 730/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 740/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 750/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 760/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 770/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 780/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 790/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 800/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 810/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 19, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 20, Batch 10/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 20/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 30/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 40/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 50/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 60/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 70/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 80/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 90/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 100/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 110/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 120/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 130/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 140/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 150/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 160/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 170/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 180/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 190/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 200/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 210/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 220/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 230/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 240/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 250/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 260/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 270/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 280/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 290/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 300/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 310/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 320/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 330/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 340/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 350/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 360/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 370/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 380/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 390/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 400/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 410/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 420/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 430/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 440/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 450/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 460/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 470/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 480/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 490/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 500/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 510/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 520/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 530/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 540/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 550/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 560/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 570/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 580/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 590/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 600/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 610/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 620/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 630/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 640/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 650/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 660/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 670/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 680/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 690/812, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 700/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 710/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 720/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 730/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 740/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 750/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 760/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 770/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 780/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 790/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 800/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 810/812, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 20, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Test Accuracy: 89.44%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if batch_idx % 10 == 9:  # Print every 10 batches\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {running_loss / 10:.4f}, Training Accuracy: {100 * correct / total:.2f}%\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation accuracy calculation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Epoch {epoch+1}, Training Accuracy: {100 * correct / total:.2f}%, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "        test_total += test_labels.size(0)\n",
    "        test_correct += (test_predicted == test_labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * test_correct / test_total:.2f}%\")\n",
    "torch.save(model.state_dict(), 'ViT_UCF50_pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
