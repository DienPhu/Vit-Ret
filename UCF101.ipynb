{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import cv2  # Assuming you have OpenCV installed\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import math\n",
    "from vit_pytorch.efficient import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "dataset = \"C:\\\\Study\\\\OJT\\\\Dataset\\\\UCF-101\"          # Dataset path\n",
    "dataset2 = \"C:\\\\Study\\\\OJT\\Dataset_Extraction\\\\UCF101\\\\Full\"               # Dataset2 path\n",
    "train_path = \"C:\\\\Study\\\\OJT\\Dataset_Extraction\\\\UCF101\\\\UCF101Train\"            # Training path\n",
    "test_path = \"C:\\\\Study\\\\OJT\\Dataset_Extraction\\\\UCF101\\\\UCF101Test\"             # Testing path\n",
    "no_of_frames = 10000                     # Total number of frames to be extracted\n",
    "categories = os.listdir(dataset)        # Name of each class/category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder C:\\Study\\OJT\\Dataset_Extraction\\UCF101\\Full already exists...\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset directory\n",
    "try:\n",
    "    os.mkdir(dataset2)\n",
    "    print(\"Folder {} created...\".format(dataset2))\n",
    "except:\n",
    "    print(\"A folder {} already exists...\".format(dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder C:\\Study\\OJT\\Dataset_Extraction\\UCF101\\UCF101Train already exists...\n"
     ]
    }
   ],
   "source": [
    "# Creating training_set directory\n",
    "try:\n",
    "    os.mkdir(train_path)\n",
    "    print(\"Folder {} created...\".format(train_path))\n",
    "except:\n",
    "    print(\"A folder {} already exists...\".format(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A folder C:\\Study\\OJT\\Dataset_Extraction\\UCF101\\UCF101Test already exists...\n"
     ]
    }
   ],
   "source": [
    "# Creating testing_set directory\n",
    "try:\n",
    "    os.mkdir(test_path)\n",
    "    print(\"Folder {} created...\".format(test_path))\n",
    "except:\n",
    "    print(\"A folder {} already exists...\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ApplyEyeMakeup created...\n",
      "Folder ApplyLipstick created...\n",
      "Folder Archery created...\n",
      "Folder BabyCrawling created...\n",
      "Folder BalanceBeam created...\n",
      "Folder BandMarching created...\n",
      "Folder BaseballPitch created...\n",
      "Folder Basketball created...\n",
      "Folder BasketballDunk created...\n",
      "Folder BenchPress created...\n",
      "Folder Biking created...\n",
      "Folder Billiards created...\n",
      "Folder BlowDryHair created...\n",
      "Folder BlowingCandles created...\n",
      "Folder BodyWeightSquats created...\n",
      "Folder Bowling created...\n",
      "Folder BoxingPunchingBag created...\n",
      "Folder BoxingSpeedBag created...\n",
      "Folder BreastStroke created...\n",
      "Folder BrushingTeeth created...\n",
      "Folder CleanAndJerk created...\n",
      "Folder CliffDiving created...\n",
      "Folder CricketBowling created...\n",
      "Folder CricketShot created...\n",
      "Folder CuttingInKitchen created...\n",
      "Folder Diving created...\n",
      "Folder Drumming created...\n",
      "Folder Fencing created...\n",
      "Folder FieldHockeyPenalty created...\n",
      "Folder FloorGymnastics created...\n",
      "Folder FrisbeeCatch created...\n",
      "Folder FrontCrawl created...\n",
      "Folder GolfSwing created...\n",
      "Folder Haircut created...\n",
      "Folder Hammering created...\n",
      "Folder HammerThrow created...\n",
      "Folder HandstandPushups created...\n",
      "Folder HandstandWalking created...\n",
      "Folder HeadMassage created...\n",
      "Folder HighJump created...\n",
      "Folder HorseRace created...\n",
      "Folder HorseRiding created...\n",
      "Folder HulaHoop created...\n",
      "Folder IceDancing created...\n",
      "Folder JavelinThrow created...\n",
      "Folder JugglingBalls created...\n",
      "Folder JumpingJack created...\n",
      "Folder JumpRope created...\n",
      "Folder Kayaking created...\n",
      "Folder Knitting created...\n",
      "Folder LongJump created...\n",
      "Folder Lunges created...\n",
      "Folder MilitaryParade created...\n",
      "Folder Mixing created...\n",
      "Folder MoppingFloor created...\n",
      "Folder Nunchucks created...\n",
      "Folder ParallelBars created...\n",
      "Folder PizzaTossing created...\n",
      "Folder PlayingCello created...\n",
      "Folder PlayingDaf created...\n",
      "Folder PlayingDhol created...\n",
      "Folder PlayingFlute created...\n",
      "Folder PlayingGuitar created...\n",
      "Folder PlayingPiano created...\n",
      "Folder PlayingSitar created...\n",
      "Folder PlayingTabla created...\n",
      "Folder PlayingViolin created...\n",
      "Folder PoleVault created...\n",
      "Folder PommelHorse created...\n",
      "Folder PullUps created...\n",
      "Folder Punch created...\n",
      "Folder PushUps created...\n",
      "Folder Rafting created...\n",
      "Folder RockClimbingIndoor created...\n",
      "Folder RopeClimbing created...\n",
      "Folder Rowing created...\n",
      "Folder SalsaSpin created...\n",
      "Folder ShavingBeard created...\n",
      "Folder Shotput created...\n",
      "Folder SkateBoarding created...\n",
      "Folder Skiing created...\n",
      "Folder Skijet created...\n",
      "Folder SkyDiving created...\n",
      "Folder SoccerJuggling created...\n",
      "Folder SoccerPenalty created...\n",
      "Folder StillRings created...\n",
      "Folder SumoWrestling created...\n",
      "Folder Surfing created...\n",
      "Folder Swing created...\n",
      "Folder TableTennisShot created...\n",
      "Folder TaiChi created...\n",
      "Folder TennisSwing created...\n",
      "Folder ThrowDiscus created...\n",
      "Folder TrampolineJumping created...\n",
      "Folder Typing created...\n",
      "Folder UnevenBars created...\n",
      "Folder VolleyballSpiking created...\n",
      "Folder WalkingWithDog created...\n",
      "Folder WallPushups created...\n",
      "Folder WritingOnBoard created...\n",
      "Folder YoYo created...\n"
     ]
    }
   ],
   "source": [
    "# Creating same directories for dataset2/ that are already present in the dataset directory\n",
    "for category in categories:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(dataset2, category))\n",
    "        print(\"Folder {} created...\".format(category))\n",
    "    except:\n",
    "        print(\"A folder already exists, named {}...\".format(category, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ApplyEyeMakeup created...\n",
      "Folder ApplyLipstick created...\n",
      "Folder Archery created...\n",
      "Folder BabyCrawling created...\n",
      "Folder BalanceBeam created...\n",
      "Folder BandMarching created...\n",
      "Folder BaseballPitch created...\n",
      "Folder Basketball created...\n",
      "Folder BasketballDunk created...\n",
      "Folder BenchPress created...\n",
      "Folder Biking created...\n",
      "Folder Billiards created...\n",
      "Folder BlowDryHair created...\n",
      "Folder BlowingCandles created...\n",
      "Folder BodyWeightSquats created...\n",
      "Folder Bowling created...\n",
      "Folder BoxingPunchingBag created...\n",
      "Folder BoxingSpeedBag created...\n",
      "Folder BreastStroke created...\n",
      "Folder BrushingTeeth created...\n",
      "Folder CleanAndJerk created...\n",
      "Folder CliffDiving created...\n",
      "Folder CricketBowling created...\n",
      "Folder CricketShot created...\n",
      "Folder CuttingInKitchen created...\n",
      "Folder Diving created...\n",
      "Folder Drumming created...\n",
      "Folder Fencing created...\n",
      "Folder FieldHockeyPenalty created...\n",
      "Folder FloorGymnastics created...\n",
      "Folder FrisbeeCatch created...\n",
      "Folder FrontCrawl created...\n",
      "Folder GolfSwing created...\n",
      "Folder Haircut created...\n",
      "Folder Hammering created...\n",
      "Folder HammerThrow created...\n",
      "Folder HandstandPushups created...\n",
      "Folder HandstandWalking created...\n",
      "Folder HeadMassage created...\n",
      "Folder HighJump created...\n",
      "Folder HorseRace created...\n",
      "Folder HorseRiding created...\n",
      "Folder HulaHoop created...\n",
      "Folder IceDancing created...\n",
      "Folder JavelinThrow created...\n",
      "Folder JugglingBalls created...\n",
      "Folder JumpingJack created...\n",
      "Folder JumpRope created...\n",
      "Folder Kayaking created...\n",
      "Folder Knitting created...\n",
      "Folder LongJump created...\n",
      "Folder Lunges created...\n",
      "Folder MilitaryParade created...\n",
      "Folder Mixing created...\n",
      "Folder MoppingFloor created...\n",
      "Folder Nunchucks created...\n",
      "Folder ParallelBars created...\n",
      "Folder PizzaTossing created...\n",
      "Folder PlayingCello created...\n",
      "Folder PlayingDaf created...\n",
      "Folder PlayingDhol created...\n",
      "Folder PlayingFlute created...\n",
      "Folder PlayingGuitar created...\n",
      "Folder PlayingPiano created...\n",
      "Folder PlayingSitar created...\n",
      "Folder PlayingTabla created...\n",
      "Folder PlayingViolin created...\n",
      "Folder PoleVault created...\n",
      "Folder PommelHorse created...\n",
      "Folder PullUps created...\n",
      "Folder Punch created...\n",
      "Folder PushUps created...\n",
      "Folder Rafting created...\n",
      "Folder RockClimbingIndoor created...\n",
      "Folder RopeClimbing created...\n",
      "Folder Rowing created...\n",
      "Folder SalsaSpin created...\n",
      "Folder ShavingBeard created...\n",
      "Folder Shotput created...\n",
      "Folder SkateBoarding created...\n",
      "Folder Skiing created...\n",
      "Folder Skijet created...\n",
      "Folder SkyDiving created...\n",
      "Folder SoccerJuggling created...\n",
      "Folder SoccerPenalty created...\n",
      "Folder StillRings created...\n",
      "Folder SumoWrestling created...\n",
      "Folder Surfing created...\n",
      "Folder Swing created...\n",
      "Folder TableTennisShot created...\n",
      "Folder TaiChi created...\n",
      "Folder TennisSwing created...\n",
      "Folder ThrowDiscus created...\n",
      "Folder TrampolineJumping created...\n",
      "Folder Typing created...\n",
      "Folder UnevenBars created...\n",
      "Folder VolleyballSpiking created...\n",
      "Folder WalkingWithDog created...\n",
      "Folder WallPushups created...\n",
      "Folder WritingOnBoard created...\n",
      "Folder YoYo created...\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(train_path, category))\n",
    "        print(\"Folder {} created...\".format(category))\n",
    "    except:\n",
    "        print(\"A folder already exists, named {}...\".format(category, train_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ApplyEyeMakeup created...\n",
      "Folder ApplyLipstick created...\n",
      "Folder Archery created...\n",
      "Folder BabyCrawling created...\n",
      "Folder BalanceBeam created...\n",
      "Folder BandMarching created...\n",
      "Folder BaseballPitch created...\n",
      "Folder Basketball created...\n",
      "Folder BasketballDunk created...\n",
      "Folder BenchPress created...\n",
      "Folder Biking created...\n",
      "Folder Billiards created...\n",
      "Folder BlowDryHair created...\n",
      "Folder BlowingCandles created...\n",
      "Folder BodyWeightSquats created...\n",
      "Folder Bowling created...\n",
      "Folder BoxingPunchingBag created...\n",
      "Folder BoxingSpeedBag created...\n",
      "Folder BreastStroke created...\n",
      "Folder BrushingTeeth created...\n",
      "Folder CleanAndJerk created...\n",
      "Folder CliffDiving created...\n",
      "Folder CricketBowling created...\n",
      "Folder CricketShot created...\n",
      "Folder CuttingInKitchen created...\n",
      "Folder Diving created...\n",
      "Folder Drumming created...\n",
      "Folder Fencing created...\n",
      "Folder FieldHockeyPenalty created...\n",
      "Folder FloorGymnastics created...\n",
      "Folder FrisbeeCatch created...\n",
      "Folder FrontCrawl created...\n",
      "Folder GolfSwing created...\n",
      "Folder Haircut created...\n",
      "Folder Hammering created...\n",
      "Folder HammerThrow created...\n",
      "Folder HandstandPushups created...\n",
      "Folder HandstandWalking created...\n",
      "Folder HeadMassage created...\n",
      "Folder HighJump created...\n",
      "Folder HorseRace created...\n",
      "Folder HorseRiding created...\n",
      "Folder HulaHoop created...\n",
      "Folder IceDancing created...\n",
      "Folder JavelinThrow created...\n",
      "Folder JugglingBalls created...\n",
      "Folder JumpingJack created...\n",
      "Folder JumpRope created...\n",
      "Folder Kayaking created...\n",
      "Folder Knitting created...\n",
      "Folder LongJump created...\n",
      "Folder Lunges created...\n",
      "Folder MilitaryParade created...\n",
      "Folder Mixing created...\n",
      "Folder MoppingFloor created...\n",
      "Folder Nunchucks created...\n",
      "Folder ParallelBars created...\n",
      "Folder PizzaTossing created...\n",
      "Folder PlayingCello created...\n",
      "Folder PlayingDaf created...\n",
      "Folder PlayingDhol created...\n",
      "Folder PlayingFlute created...\n",
      "Folder PlayingGuitar created...\n",
      "Folder PlayingPiano created...\n",
      "Folder PlayingSitar created...\n",
      "Folder PlayingTabla created...\n",
      "Folder PlayingViolin created...\n",
      "Folder PoleVault created...\n",
      "Folder PommelHorse created...\n",
      "Folder PullUps created...\n",
      "Folder Punch created...\n",
      "Folder PushUps created...\n",
      "Folder Rafting created...\n",
      "Folder RockClimbingIndoor created...\n",
      "Folder RopeClimbing created...\n",
      "Folder Rowing created...\n",
      "Folder SalsaSpin created...\n",
      "Folder ShavingBeard created...\n",
      "Folder Shotput created...\n",
      "Folder SkateBoarding created...\n",
      "Folder Skiing created...\n",
      "Folder Skijet created...\n",
      "Folder SkyDiving created...\n",
      "Folder SoccerJuggling created...\n",
      "Folder SoccerPenalty created...\n",
      "Folder StillRings created...\n",
      "Folder SumoWrestling created...\n",
      "Folder Surfing created...\n",
      "Folder Swing created...\n",
      "Folder TableTennisShot created...\n",
      "Folder TaiChi created...\n",
      "Folder TennisSwing created...\n",
      "Folder ThrowDiscus created...\n",
      "Folder TrampolineJumping created...\n",
      "Folder Typing created...\n",
      "Folder UnevenBars created...\n",
      "Folder VolleyballSpiking created...\n",
      "Folder WalkingWithDog created...\n",
      "Folder WallPushups created...\n",
      "Folder WritingOnBoard created...\n",
      "Folder YoYo created...\n"
     ]
    }
   ],
   "source": [
    "# Creating same directories for testing_set/ that are already present in the dataset directory\n",
    "for category in categories:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(test_path, category))\n",
    "        print(\"Folder {} created...\".format(category))\n",
    "    except:\n",
    "        print(\"A folder already exists, named {}...\".format(category, test_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90a0585ae73435c9b90352db17d6e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combining multiple videos into single video file\n",
    "for category in tqdm(categories):\n",
    "    videofiles = glob.glob(os.path.join(dataset, category, \"**/*.avi\"), recursive=True)\n",
    "    if videofiles:\n",
    "        cap = cv2.VideoCapture(videofiles[0])\n",
    "    else:\n",
    "        print(\"No video files found in {}/{}\".format(dataset, category))\n",
    "    video_index = 0\n",
    "    cap = cv2.VideoCapture(videofiles[0])    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    # fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(\"{}/{}/{}.avi\".format(dataset2, category, category), fourcc, 25, (320, 240))\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if frame is None:\n",
    "            video_index += 1\n",
    "            if video_index >= len(videofiles):\n",
    "                break\n",
    "            else:\n",
    "                cap = cv2.VideoCapture(videofiles[ video_index ])\n",
    "                ret, frame = cap.read()\n",
    "                out.write(frame)\n",
    "        else:\n",
    "            out.write(frame)\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fc8beb750d48028103a3476290bb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24358, 19761, 23690, 21740, 12822, 32365, 14944, 18497, 9954, 18966, 30324, 42936, 30289, 11179, 14356, 19519, 31260, 31835, 18340, 33005, 21216, 12132, 13495, 15844, 18811, 25841, 46503, 14570, 14751, 18887, 11947, 23001, 22895, 21748, 17701, 26019, 14842, 18505, 26896, 13796, 31785, 31361, 13084, 39338, 11855, 30115, 10387, 63340, 28035, 26124, 17487, 28920, 15747, 15704, 23093, 30592, 16290, 13868, 37671, 44077, 47926, 41461, 39933, 23076, 56258, 30210, 18383, 25083, 40890, 13778, 43334, 8404, 21145, 55036, 19649, 45329, 29090, 35660, 13004, 16486, 29146, 22995, 20818, 43208, 13159, 20736, 22721, 25320, 22108, 23179, 18256, 26376, 12524, 24002, 26694, 15884, 12836, 27697, 14391, 24312, 23350]\n"
     ]
    }
   ],
   "source": [
    "# Saving total no. of frames of each classes/categories into an array\n",
    "total_frames = []\n",
    "for category in tqdm(categories):\n",
    "    cap = cv2.VideoCapture(dataset2 + \"/\" + category + \"/\" + category + \".avi\")\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_frames.append(length)\n",
    "    cap.release()\n",
    "    \n",
    "print(total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78d387ed2694e57936b1b01a764838a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting one frame per five frame from the Videos\n",
    "for category in tqdm(categories):\n",
    "    count = 0    \n",
    "    a = glob.glob(dataset2 + '/' + category + '/' + category +'.avi')\n",
    "    for i in range(len(a)):\n",
    "        cap = cv2.VideoCapture(a[i])\n",
    "        frameRate = cap.get(5)\n",
    "        while(cap.isOpened()):\n",
    "            frameId = cap.get(1)\n",
    "            ret, frame = cap.read()\n",
    "            if (ret != True):\n",
    "                break\n",
    "            if (frameId % math.floor(frameRate) == 0):\n",
    "                cv2.imwrite(train_path + '/' + category + '/{}_{}.jpg'.format(category, count), frame)\n",
    "                count += 1\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf526d04ff94118b8753e53ade2a296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Moving 150 random images from training_set into testing_set\n",
    "for category in tqdm(categories):\n",
    "    sub_file = [file for file in glob.glob(train_path +'/'+ category +'/'+ \"*.jpg\")]\n",
    "    test_files = random.sample(sub_file, 50)\n",
    "    for test_file in test_files:\n",
    "        img = cv2.imread(test_file)\n",
    "        os.remove(test_file)\n",
    "        test_filename = os.path.basename(test_file)\n",
    "        cv2.imwrite(test_path +'/' + category + '/' + test_filename , img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for loading and preprocessing images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),  # You can adjust the crop size as needed\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset for the entire training set\n",
    "full_dataset = ImageFolder(train_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "val_size = len(full_dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom datasets for training and test\n",
    "train_dataset = ImageFolder(train_path, transform=transform)\n",
    "test_dataset = ImageFolder(test_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for training and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_transformer = Linformer(\n",
    "    dim=128,\n",
    "    seq_len=49+1,  # 7x7 patches + 1 cls-token\n",
    "    depth=12,\n",
    "    heads=8,\n",
    "    k=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    dim=128,\n",
    "    image_size=224,\n",
    "    patch_size=32,\n",
    "    num_classes=101,\n",
    "    transformer=efficient_transformer,\n",
    "    channels=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10/1474, Loss: 4.6827, Training Accuracy: 1.09%\n",
      "Epoch 1, Batch 20/1474, Loss: 4.6292, Training Accuracy: 1.56%\n",
      "Epoch 1, Batch 30/1474, Loss: 4.4403, Training Accuracy: 1.98%\n",
      "Epoch 1, Batch 40/1474, Loss: 4.3247, Training Accuracy: 2.89%\n",
      "Epoch 1, Batch 50/1474, Loss: 4.2538, Training Accuracy: 3.84%\n",
      "Epoch 1, Batch 60/1474, Loss: 4.1482, Training Accuracy: 4.61%\n",
      "Epoch 1, Batch 70/1474, Loss: 4.0476, Training Accuracy: 5.07%\n",
      "Epoch 1, Batch 80/1474, Loss: 3.9504, Training Accuracy: 5.80%\n",
      "Epoch 1, Batch 90/1474, Loss: 3.8775, Training Accuracy: 6.49%\n",
      "Epoch 1, Batch 100/1474, Loss: 3.9551, Training Accuracy: 6.77%\n",
      "Epoch 1, Batch 110/1474, Loss: 3.8227, Training Accuracy: 7.39%\n",
      "Epoch 1, Batch 120/1474, Loss: 3.7127, Training Accuracy: 7.89%\n",
      "Epoch 1, Batch 130/1474, Loss: 3.7077, Training Accuracy: 8.40%\n",
      "Epoch 1, Batch 140/1474, Loss: 3.6892, Training Accuracy: 8.73%\n",
      "Epoch 1, Batch 150/1474, Loss: 3.5938, Training Accuracy: 9.17%\n",
      "Epoch 1, Batch 160/1474, Loss: 3.5485, Training Accuracy: 9.79%\n",
      "Epoch 1, Batch 170/1474, Loss: 3.5182, Training Accuracy: 10.31%\n",
      "Epoch 1, Batch 180/1474, Loss: 3.5354, Training Accuracy: 10.81%\n",
      "Epoch 1, Batch 190/1474, Loss: 3.4579, Training Accuracy: 11.37%\n",
      "Epoch 1, Batch 200/1474, Loss: 3.3715, Training Accuracy: 11.89%\n",
      "Epoch 1, Batch 210/1474, Loss: 3.2628, Training Accuracy: 12.60%\n",
      "Epoch 1, Batch 220/1474, Loss: 3.3274, Training Accuracy: 13.12%\n",
      "Epoch 1, Batch 230/1474, Loss: 3.2024, Training Accuracy: 13.57%\n",
      "Epoch 1, Batch 240/1474, Loss: 3.2044, Training Accuracy: 14.03%\n",
      "Epoch 1, Batch 250/1474, Loss: 3.0859, Training Accuracy: 14.58%\n",
      "Epoch 1, Batch 260/1474, Loss: 3.0421, Training Accuracy: 15.12%\n",
      "Epoch 1, Batch 270/1474, Loss: 3.0377, Training Accuracy: 15.63%\n",
      "Epoch 1, Batch 280/1474, Loss: 3.0046, Training Accuracy: 16.13%\n",
      "Epoch 1, Batch 290/1474, Loss: 2.9641, Training Accuracy: 16.54%\n",
      "Epoch 1, Batch 300/1474, Loss: 2.9425, Training Accuracy: 17.05%\n",
      "Epoch 1, Batch 310/1474, Loss: 2.9039, Training Accuracy: 17.56%\n",
      "Epoch 1, Batch 320/1474, Loss: 2.9109, Training Accuracy: 18.08%\n",
      "Epoch 1, Batch 330/1474, Loss: 2.9364, Training Accuracy: 18.53%\n",
      "Epoch 1, Batch 340/1474, Loss: 2.7739, Training Accuracy: 19.00%\n",
      "Epoch 1, Batch 350/1474, Loss: 2.7196, Training Accuracy: 19.54%\n",
      "Epoch 1, Batch 360/1474, Loss: 2.7290, Training Accuracy: 20.03%\n",
      "Epoch 1, Batch 370/1474, Loss: 2.6774, Training Accuracy: 20.48%\n",
      "Epoch 1, Batch 380/1474, Loss: 2.6651, Training Accuracy: 20.92%\n",
      "Epoch 1, Batch 390/1474, Loss: 2.6661, Training Accuracy: 21.33%\n",
      "Epoch 1, Batch 400/1474, Loss: 2.5375, Training Accuracy: 21.84%\n",
      "Epoch 1, Batch 410/1474, Loss: 2.5163, Training Accuracy: 22.27%\n",
      "Epoch 1, Batch 420/1474, Loss: 2.5708, Training Accuracy: 22.73%\n",
      "Epoch 1, Batch 430/1474, Loss: 2.4220, Training Accuracy: 23.23%\n",
      "Epoch 1, Batch 440/1474, Loss: 2.5595, Training Accuracy: 23.60%\n",
      "Epoch 1, Batch 450/1474, Loss: 2.4542, Training Accuracy: 24.04%\n",
      "Epoch 1, Batch 460/1474, Loss: 2.4786, Training Accuracy: 24.43%\n",
      "Epoch 1, Batch 470/1474, Loss: 2.4155, Training Accuracy: 24.83%\n",
      "Epoch 1, Batch 480/1474, Loss: 2.3512, Training Accuracy: 25.25%\n",
      "Epoch 1, Batch 490/1474, Loss: 2.5243, Training Accuracy: 25.57%\n",
      "Epoch 1, Batch 500/1474, Loss: 2.4213, Training Accuracy: 25.93%\n",
      "Epoch 1, Batch 510/1474, Loss: 2.2388, Training Accuracy: 26.36%\n",
      "Epoch 1, Batch 520/1474, Loss: 2.2174, Training Accuracy: 26.78%\n",
      "Epoch 1, Batch 530/1474, Loss: 2.1987, Training Accuracy: 27.17%\n",
      "Epoch 1, Batch 540/1474, Loss: 2.2767, Training Accuracy: 27.57%\n",
      "Epoch 1, Batch 550/1474, Loss: 2.1860, Training Accuracy: 27.95%\n",
      "Epoch 1, Batch 560/1474, Loss: 2.2256, Training Accuracy: 28.28%\n",
      "Epoch 1, Batch 570/1474, Loss: 2.0212, Training Accuracy: 28.69%\n",
      "Epoch 1, Batch 580/1474, Loss: 2.1099, Training Accuracy: 29.07%\n",
      "Epoch 1, Batch 590/1474, Loss: 2.0887, Training Accuracy: 29.45%\n",
      "Epoch 1, Batch 600/1474, Loss: 2.0825, Training Accuracy: 29.81%\n",
      "Epoch 1, Batch 610/1474, Loss: 2.0260, Training Accuracy: 30.18%\n",
      "Epoch 1, Batch 620/1474, Loss: 2.0562, Training Accuracy: 30.55%\n",
      "Epoch 1, Batch 630/1474, Loss: 2.0834, Training Accuracy: 30.87%\n",
      "Epoch 1, Batch 640/1474, Loss: 2.0683, Training Accuracy: 31.17%\n",
      "Epoch 1, Batch 650/1474, Loss: 2.0766, Training Accuracy: 31.46%\n",
      "Epoch 1, Batch 660/1474, Loss: 2.0666, Training Accuracy: 31.76%\n",
      "Epoch 1, Batch 670/1474, Loss: 1.9141, Training Accuracy: 32.09%\n",
      "Epoch 1, Batch 680/1474, Loss: 2.0222, Training Accuracy: 32.36%\n",
      "Epoch 1, Batch 690/1474, Loss: 1.9685, Training Accuracy: 32.67%\n",
      "Epoch 1, Batch 700/1474, Loss: 1.9063, Training Accuracy: 32.98%\n",
      "Epoch 1, Batch 710/1474, Loss: 1.9177, Training Accuracy: 33.27%\n",
      "Epoch 1, Batch 720/1474, Loss: 1.8349, Training Accuracy: 33.59%\n",
      "Epoch 1, Batch 730/1474, Loss: 1.8300, Training Accuracy: 33.88%\n",
      "Epoch 1, Batch 740/1474, Loss: 1.8666, Training Accuracy: 34.18%\n",
      "Epoch 1, Batch 750/1474, Loss: 1.8052, Training Accuracy: 34.46%\n",
      "Epoch 1, Batch 760/1474, Loss: 1.9985, Training Accuracy: 34.70%\n",
      "Epoch 1, Batch 770/1474, Loss: 1.7133, Training Accuracy: 35.01%\n",
      "Epoch 1, Batch 780/1474, Loss: 1.9241, Training Accuracy: 35.23%\n",
      "Epoch 1, Batch 790/1474, Loss: 1.8224, Training Accuracy: 35.50%\n",
      "Epoch 1, Batch 800/1474, Loss: 1.7454, Training Accuracy: 35.78%\n",
      "Epoch 1, Batch 810/1474, Loss: 1.7555, Training Accuracy: 36.02%\n",
      "Epoch 1, Batch 820/1474, Loss: 1.7893, Training Accuracy: 36.29%\n",
      "Epoch 1, Batch 830/1474, Loss: 1.7955, Training Accuracy: 36.55%\n",
      "Epoch 1, Batch 840/1474, Loss: 1.6881, Training Accuracy: 36.83%\n",
      "Epoch 1, Batch 850/1474, Loss: 1.7932, Training Accuracy: 37.08%\n",
      "Epoch 1, Batch 860/1474, Loss: 1.6401, Training Accuracy: 37.34%\n",
      "Epoch 1, Batch 870/1474, Loss: 1.7998, Training Accuracy: 37.53%\n",
      "Epoch 1, Batch 880/1474, Loss: 1.6688, Training Accuracy: 37.78%\n",
      "Epoch 1, Batch 890/1474, Loss: 1.6334, Training Accuracy: 38.03%\n",
      "Epoch 1, Batch 900/1474, Loss: 1.6675, Training Accuracy: 38.26%\n",
      "Epoch 1, Batch 910/1474, Loss: 1.7368, Training Accuracy: 38.46%\n",
      "Epoch 1, Batch 920/1474, Loss: 1.6255, Training Accuracy: 38.71%\n",
      "Epoch 1, Batch 930/1474, Loss: 1.5969, Training Accuracy: 38.98%\n",
      "Epoch 1, Batch 940/1474, Loss: 1.5862, Training Accuracy: 39.22%\n",
      "Epoch 1, Batch 950/1474, Loss: 1.5245, Training Accuracy: 39.46%\n",
      "Epoch 1, Batch 960/1474, Loss: 1.7133, Training Accuracy: 39.66%\n",
      "Epoch 1, Batch 970/1474, Loss: 1.5090, Training Accuracy: 39.92%\n",
      "Epoch 1, Batch 980/1474, Loss: 1.4975, Training Accuracy: 40.14%\n",
      "Epoch 1, Batch 990/1474, Loss: 1.6617, Training Accuracy: 40.35%\n",
      "Epoch 1, Batch 1000/1474, Loss: 1.6894, Training Accuracy: 40.50%\n",
      "Epoch 1, Batch 1010/1474, Loss: 1.4268, Training Accuracy: 40.74%\n",
      "Epoch 1, Batch 1020/1474, Loss: 1.4414, Training Accuracy: 40.97%\n",
      "Epoch 1, Batch 1030/1474, Loss: 1.5350, Training Accuracy: 41.17%\n",
      "Epoch 1, Batch 1040/1474, Loss: 1.5113, Training Accuracy: 41.38%\n",
      "Epoch 1, Batch 1050/1474, Loss: 1.4199, Training Accuracy: 41.61%\n",
      "Epoch 1, Batch 1060/1474, Loss: 1.4312, Training Accuracy: 41.82%\n",
      "Epoch 1, Batch 1070/1474, Loss: 1.5313, Training Accuracy: 42.00%\n",
      "Epoch 1, Batch 1080/1474, Loss: 1.2895, Training Accuracy: 42.25%\n",
      "Epoch 1, Batch 1090/1474, Loss: 1.4259, Training Accuracy: 42.48%\n",
      "Epoch 1, Batch 1100/1474, Loss: 1.4296, Training Accuracy: 42.68%\n",
      "Epoch 1, Batch 1110/1474, Loss: 1.4351, Training Accuracy: 42.88%\n",
      "Epoch 1, Batch 1120/1474, Loss: 1.5049, Training Accuracy: 43.07%\n",
      "Epoch 1, Batch 1130/1474, Loss: 1.4890, Training Accuracy: 43.23%\n",
      "Epoch 1, Batch 1140/1474, Loss: 1.3898, Training Accuracy: 43.42%\n",
      "Epoch 1, Batch 1150/1474, Loss: 1.3332, Training Accuracy: 43.62%\n",
      "Epoch 1, Batch 1160/1474, Loss: 1.4504, Training Accuracy: 43.80%\n",
      "Epoch 1, Batch 1170/1474, Loss: 1.4499, Training Accuracy: 44.00%\n",
      "Epoch 1, Batch 1180/1474, Loss: 1.4341, Training Accuracy: 44.18%\n",
      "Epoch 1, Batch 1190/1474, Loss: 1.3730, Training Accuracy: 44.37%\n",
      "Epoch 1, Batch 1200/1474, Loss: 1.4267, Training Accuracy: 44.54%\n",
      "Epoch 1, Batch 1210/1474, Loss: 1.3694, Training Accuracy: 44.72%\n",
      "Epoch 1, Batch 1220/1474, Loss: 1.4159, Training Accuracy: 44.90%\n",
      "Epoch 1, Batch 1230/1474, Loss: 1.2644, Training Accuracy: 45.10%\n",
      "Epoch 1, Batch 1240/1474, Loss: 1.4770, Training Accuracy: 45.25%\n",
      "Epoch 1, Batch 1250/1474, Loss: 1.3088, Training Accuracy: 45.43%\n",
      "Epoch 1, Batch 1260/1474, Loss: 1.2477, Training Accuracy: 45.62%\n",
      "Epoch 1, Batch 1270/1474, Loss: 1.3468, Training Accuracy: 45.79%\n",
      "Epoch 1, Batch 1280/1474, Loss: 1.2895, Training Accuracy: 45.98%\n",
      "Epoch 1, Batch 1290/1474, Loss: 1.2686, Training Accuracy: 46.15%\n",
      "Epoch 1, Batch 1300/1474, Loss: 1.2307, Training Accuracy: 46.34%\n",
      "Epoch 1, Batch 1310/1474, Loss: 1.1468, Training Accuracy: 46.53%\n",
      "Epoch 1, Batch 1320/1474, Loss: 1.1645, Training Accuracy: 46.71%\n",
      "Epoch 1, Batch 1330/1474, Loss: 1.2708, Training Accuracy: 46.87%\n",
      "Epoch 1, Batch 1340/1474, Loss: 1.3146, Training Accuracy: 47.01%\n",
      "Epoch 1, Batch 1350/1474, Loss: 1.2451, Training Accuracy: 47.16%\n",
      "Epoch 1, Batch 1360/1474, Loss: 1.3836, Training Accuracy: 47.30%\n",
      "Epoch 1, Batch 1370/1474, Loss: 1.1489, Training Accuracy: 47.46%\n",
      "Epoch 1, Batch 1380/1474, Loss: 1.1858, Training Accuracy: 47.64%\n",
      "Epoch 1, Batch 1390/1474, Loss: 1.1672, Training Accuracy: 47.80%\n",
      "Epoch 1, Batch 1400/1474, Loss: 1.1083, Training Accuracy: 47.97%\n",
      "Epoch 1, Batch 1410/1474, Loss: 1.2196, Training Accuracy: 48.14%\n",
      "Epoch 1, Batch 1420/1474, Loss: 1.1295, Training Accuracy: 48.31%\n",
      "Epoch 1, Batch 1430/1474, Loss: 1.1591, Training Accuracy: 48.46%\n",
      "Epoch 1, Batch 1440/1474, Loss: 1.1734, Training Accuracy: 48.61%\n",
      "Epoch 1, Batch 1450/1474, Loss: 1.0695, Training Accuracy: 48.79%\n",
      "Epoch 1, Batch 1460/1474, Loss: 1.1950, Training Accuracy: 48.93%\n",
      "Epoch 1, Batch 1470/1474, Loss: 1.2036, Training Accuracy: 49.08%\n",
      "Epoch 1, Training Accuracy: 49.13%, Validation Accuracy: 74.73%\n",
      "Epoch 2, Batch 10/1474, Loss: 0.9334, Training Accuracy: 75.62%\n",
      "Epoch 2, Batch 20/1474, Loss: 0.9597, Training Accuracy: 75.39%\n",
      "Epoch 2, Batch 30/1474, Loss: 0.9543, Training Accuracy: 75.00%\n",
      "Epoch 2, Batch 40/1474, Loss: 0.8926, Training Accuracy: 75.35%\n",
      "Epoch 2, Batch 50/1474, Loss: 0.8299, Training Accuracy: 76.25%\n",
      "Epoch 2, Batch 60/1474, Loss: 0.9592, Training Accuracy: 76.30%\n",
      "Epoch 2, Batch 70/1474, Loss: 0.8137, Training Accuracy: 76.70%\n",
      "Epoch 2, Batch 80/1474, Loss: 0.8392, Training Accuracy: 76.88%\n",
      "Epoch 2, Batch 90/1474, Loss: 0.7989, Training Accuracy: 77.26%\n",
      "Epoch 2, Batch 100/1474, Loss: 0.8807, Training Accuracy: 77.58%\n",
      "Epoch 2, Batch 110/1474, Loss: 0.8410, Training Accuracy: 77.66%\n",
      "Epoch 2, Batch 120/1474, Loss: 0.8779, Training Accuracy: 77.63%\n",
      "Epoch 2, Batch 130/1474, Loss: 0.8428, Training Accuracy: 77.73%\n",
      "Epoch 2, Batch 140/1474, Loss: 0.8690, Training Accuracy: 77.73%\n",
      "Epoch 2, Batch 150/1474, Loss: 0.8881, Training Accuracy: 77.78%\n",
      "Epoch 2, Batch 160/1474, Loss: 0.7660, Training Accuracy: 77.92%\n",
      "Epoch 2, Batch 170/1474, Loss: 0.7897, Training Accuracy: 77.99%\n",
      "Epoch 2, Batch 180/1474, Loss: 0.7730, Training Accuracy: 78.22%\n",
      "Epoch 2, Batch 190/1474, Loss: 0.8268, Training Accuracy: 78.31%\n",
      "Epoch 2, Batch 200/1474, Loss: 0.8059, Training Accuracy: 78.38%\n",
      "Epoch 2, Batch 210/1474, Loss: 0.7431, Training Accuracy: 78.50%\n",
      "Epoch 2, Batch 220/1474, Loss: 0.7598, Training Accuracy: 78.59%\n",
      "Epoch 2, Batch 230/1474, Loss: 0.8673, Training Accuracy: 78.49%\n",
      "Epoch 2, Batch 240/1474, Loss: 0.8351, Training Accuracy: 78.51%\n",
      "Epoch 2, Batch 250/1474, Loss: 0.7104, Training Accuracy: 78.66%\n",
      "Epoch 2, Batch 260/1474, Loss: 0.7263, Training Accuracy: 78.74%\n",
      "Epoch 2, Batch 270/1474, Loss: 0.8007, Training Accuracy: 78.77%\n",
      "Epoch 2, Batch 280/1474, Loss: 0.7808, Training Accuracy: 78.78%\n",
      "Epoch 2, Batch 290/1474, Loss: 0.7964, Training Accuracy: 78.83%\n",
      "Epoch 2, Batch 300/1474, Loss: 0.8686, Training Accuracy: 78.80%\n",
      "Epoch 2, Batch 310/1474, Loss: 0.7469, Training Accuracy: 78.89%\n",
      "Epoch 2, Batch 320/1474, Loss: 0.7876, Training Accuracy: 78.93%\n",
      "Epoch 2, Batch 330/1474, Loss: 0.7268, Training Accuracy: 78.99%\n",
      "Epoch 2, Batch 340/1474, Loss: 0.7292, Training Accuracy: 79.07%\n",
      "Epoch 2, Batch 350/1474, Loss: 0.7179, Training Accuracy: 79.10%\n",
      "Epoch 2, Batch 360/1474, Loss: 0.7408, Training Accuracy: 79.15%\n",
      "Epoch 2, Batch 370/1474, Loss: 0.7303, Training Accuracy: 79.22%\n",
      "Epoch 2, Batch 380/1474, Loss: 0.7616, Training Accuracy: 79.27%\n",
      "Epoch 2, Batch 390/1474, Loss: 0.8638, Training Accuracy: 79.21%\n",
      "Epoch 2, Batch 400/1474, Loss: 0.7888, Training Accuracy: 79.23%\n",
      "Epoch 2, Batch 410/1474, Loss: 0.7385, Training Accuracy: 79.26%\n",
      "Epoch 2, Batch 420/1474, Loss: 0.6396, Training Accuracy: 79.37%\n",
      "Epoch 2, Batch 430/1474, Loss: 0.7778, Training Accuracy: 79.39%\n",
      "Epoch 2, Batch 440/1474, Loss: 0.7581, Training Accuracy: 79.45%\n",
      "Epoch 2, Batch 450/1474, Loss: 0.7507, Training Accuracy: 79.47%\n",
      "Epoch 2, Batch 460/1474, Loss: 0.7270, Training Accuracy: 79.56%\n",
      "Epoch 2, Batch 470/1474, Loss: 0.7803, Training Accuracy: 79.55%\n",
      "Epoch 2, Batch 480/1474, Loss: 0.7049, Training Accuracy: 79.62%\n",
      "Epoch 2, Batch 490/1474, Loss: 0.6881, Training Accuracy: 79.67%\n",
      "Epoch 2, Batch 500/1474, Loss: 0.7802, Training Accuracy: 79.66%\n",
      "Epoch 2, Batch 510/1474, Loss: 0.7648, Training Accuracy: 79.68%\n",
      "Epoch 2, Batch 520/1474, Loss: 0.7883, Training Accuracy: 79.69%\n",
      "Epoch 2, Batch 530/1474, Loss: 0.7260, Training Accuracy: 79.72%\n",
      "Epoch 2, Batch 540/1474, Loss: 0.6922, Training Accuracy: 79.76%\n",
      "Epoch 2, Batch 550/1474, Loss: 0.6084, Training Accuracy: 79.84%\n",
      "Epoch 2, Batch 560/1474, Loss: 0.7264, Training Accuracy: 79.89%\n",
      "Epoch 2, Batch 570/1474, Loss: 0.7065, Training Accuracy: 79.93%\n",
      "Epoch 2, Batch 580/1474, Loss: 0.6871, Training Accuracy: 79.97%\n",
      "Epoch 2, Batch 590/1474, Loss: 0.6803, Training Accuracy: 80.01%\n",
      "Epoch 2, Batch 600/1474, Loss: 0.6386, Training Accuracy: 80.06%\n",
      "Epoch 2, Batch 610/1474, Loss: 0.7447, Training Accuracy: 80.07%\n",
      "Epoch 2, Batch 620/1474, Loss: 0.7973, Training Accuracy: 80.04%\n",
      "Epoch 2, Batch 630/1474, Loss: 0.6771, Training Accuracy: 80.05%\n",
      "Epoch 2, Batch 640/1474, Loss: 0.7166, Training Accuracy: 80.08%\n",
      "Epoch 2, Batch 650/1474, Loss: 0.7670, Training Accuracy: 80.09%\n",
      "Epoch 2, Batch 660/1474, Loss: 0.6308, Training Accuracy: 80.13%\n",
      "Epoch 2, Batch 670/1474, Loss: 0.6273, Training Accuracy: 80.17%\n",
      "Epoch 2, Batch 680/1474, Loss: 0.7149, Training Accuracy: 80.20%\n",
      "Epoch 2, Batch 690/1474, Loss: 0.6572, Training Accuracy: 80.24%\n",
      "Epoch 2, Batch 700/1474, Loss: 0.7142, Training Accuracy: 80.25%\n",
      "Epoch 2, Batch 710/1474, Loss: 0.6799, Training Accuracy: 80.26%\n",
      "Epoch 2, Batch 720/1474, Loss: 0.7350, Training Accuracy: 80.29%\n",
      "Epoch 2, Batch 730/1474, Loss: 0.7807, Training Accuracy: 80.28%\n",
      "Epoch 2, Batch 740/1474, Loss: 0.7095, Training Accuracy: 80.31%\n",
      "Epoch 2, Batch 750/1474, Loss: 0.6974, Training Accuracy: 80.31%\n",
      "Epoch 2, Batch 760/1474, Loss: 0.6746, Training Accuracy: 80.36%\n",
      "Epoch 2, Batch 770/1474, Loss: 0.6012, Training Accuracy: 80.42%\n",
      "Epoch 2, Batch 780/1474, Loss: 0.6304, Training Accuracy: 80.45%\n",
      "Epoch 2, Batch 790/1474, Loss: 0.6487, Training Accuracy: 80.45%\n",
      "Epoch 2, Batch 800/1474, Loss: 0.6589, Training Accuracy: 80.48%\n",
      "Epoch 2, Batch 810/1474, Loss: 0.6301, Training Accuracy: 80.51%\n",
      "Epoch 2, Batch 820/1474, Loss: 0.6289, Training Accuracy: 80.54%\n",
      "Epoch 2, Batch 830/1474, Loss: 0.6562, Training Accuracy: 80.57%\n",
      "Epoch 2, Batch 840/1474, Loss: 0.6874, Training Accuracy: 80.60%\n",
      "Epoch 2, Batch 850/1474, Loss: 0.6100, Training Accuracy: 80.64%\n",
      "Epoch 2, Batch 860/1474, Loss: 0.6998, Training Accuracy: 80.64%\n",
      "Epoch 2, Batch 870/1474, Loss: 0.6803, Training Accuracy: 80.67%\n",
      "Epoch 2, Batch 880/1474, Loss: 0.7993, Training Accuracy: 80.66%\n",
      "Epoch 2, Batch 890/1474, Loss: 0.7002, Training Accuracy: 80.66%\n",
      "Epoch 2, Batch 900/1474, Loss: 0.7863, Training Accuracy: 80.64%\n",
      "Epoch 2, Batch 910/1474, Loss: 0.5873, Training Accuracy: 80.68%\n",
      "Epoch 2, Batch 920/1474, Loss: 0.6147, Training Accuracy: 80.71%\n",
      "Epoch 2, Batch 930/1474, Loss: 0.6324, Training Accuracy: 80.73%\n",
      "Epoch 2, Batch 940/1474, Loss: 0.5752, Training Accuracy: 80.78%\n",
      "Epoch 2, Batch 950/1474, Loss: 0.6719, Training Accuracy: 80.79%\n",
      "Epoch 2, Batch 960/1474, Loss: 0.5983, Training Accuracy: 80.83%\n",
      "Epoch 2, Batch 970/1474, Loss: 0.6241, Training Accuracy: 80.87%\n",
      "Epoch 2, Batch 980/1474, Loss: 0.6940, Training Accuracy: 80.87%\n",
      "Epoch 2, Batch 990/1474, Loss: 0.6011, Training Accuracy: 80.89%\n",
      "Epoch 2, Batch 1000/1474, Loss: 0.5522, Training Accuracy: 80.94%\n",
      "Epoch 2, Batch 1010/1474, Loss: 0.6519, Training Accuracy: 80.96%\n",
      "Epoch 2, Batch 1020/1474, Loss: 0.6349, Training Accuracy: 80.98%\n",
      "Epoch 2, Batch 1030/1474, Loss: 0.6998, Training Accuracy: 80.98%\n",
      "Epoch 2, Batch 1040/1474, Loss: 0.6447, Training Accuracy: 81.01%\n",
      "Epoch 2, Batch 1050/1474, Loss: 0.7331, Training Accuracy: 81.02%\n",
      "Epoch 2, Batch 1060/1474, Loss: 0.5833, Training Accuracy: 81.08%\n",
      "Epoch 2, Batch 1070/1474, Loss: 0.6872, Training Accuracy: 81.09%\n",
      "Epoch 2, Batch 1080/1474, Loss: 0.6227, Training Accuracy: 81.11%\n",
      "Epoch 2, Batch 1090/1474, Loss: 0.6955, Training Accuracy: 81.13%\n",
      "Epoch 2, Batch 1100/1474, Loss: 0.6721, Training Accuracy: 81.14%\n",
      "Epoch 2, Batch 1110/1474, Loss: 0.5986, Training Accuracy: 81.16%\n",
      "Epoch 2, Batch 1120/1474, Loss: 0.6637, Training Accuracy: 81.18%\n",
      "Epoch 2, Batch 1130/1474, Loss: 0.5946, Training Accuracy: 81.20%\n",
      "Epoch 2, Batch 1140/1474, Loss: 0.6190, Training Accuracy: 81.23%\n",
      "Epoch 2, Batch 1150/1474, Loss: 0.6186, Training Accuracy: 81.26%\n",
      "Epoch 2, Batch 1160/1474, Loss: 0.6251, Training Accuracy: 81.28%\n",
      "Epoch 2, Batch 1170/1474, Loss: 0.5829, Training Accuracy: 81.30%\n",
      "Epoch 2, Batch 1180/1474, Loss: 0.5830, Training Accuracy: 81.33%\n",
      "Epoch 2, Batch 1190/1474, Loss: 0.6307, Training Accuracy: 81.35%\n",
      "Epoch 2, Batch 1200/1474, Loss: 0.5796, Training Accuracy: 81.37%\n",
      "Epoch 2, Batch 1210/1474, Loss: 0.5418, Training Accuracy: 81.41%\n",
      "Epoch 2, Batch 1220/1474, Loss: 0.6429, Training Accuracy: 81.43%\n",
      "Epoch 2, Batch 1230/1474, Loss: 0.6300, Training Accuracy: 81.44%\n",
      "Epoch 2, Batch 1240/1474, Loss: 0.6122, Training Accuracy: 81.46%\n",
      "Epoch 2, Batch 1250/1474, Loss: 0.5773, Training Accuracy: 81.50%\n",
      "Epoch 2, Batch 1260/1474, Loss: 0.5642, Training Accuracy: 81.51%\n",
      "Epoch 2, Batch 1270/1474, Loss: 0.5478, Training Accuracy: 81.56%\n",
      "Epoch 2, Batch 1280/1474, Loss: 0.6083, Training Accuracy: 81.58%\n",
      "Epoch 2, Batch 1290/1474, Loss: 0.5490, Training Accuracy: 81.61%\n",
      "Epoch 2, Batch 1300/1474, Loss: 0.5387, Training Accuracy: 81.65%\n",
      "Epoch 2, Batch 1310/1474, Loss: 0.5515, Training Accuracy: 81.67%\n",
      "Epoch 2, Batch 1320/1474, Loss: 0.5408, Training Accuracy: 81.71%\n",
      "Epoch 2, Batch 1330/1474, Loss: 0.5972, Training Accuracy: 81.73%\n",
      "Epoch 2, Batch 1340/1474, Loss: 0.5311, Training Accuracy: 81.77%\n",
      "Epoch 2, Batch 1350/1474, Loss: 0.5483, Training Accuracy: 81.79%\n",
      "Epoch 2, Batch 1360/1474, Loss: 0.5588, Training Accuracy: 81.82%\n",
      "Epoch 2, Batch 1370/1474, Loss: 0.5085, Training Accuracy: 81.85%\n",
      "Epoch 2, Batch 1380/1474, Loss: 0.5374, Training Accuracy: 81.89%\n",
      "Epoch 2, Batch 1390/1474, Loss: 0.5345, Training Accuracy: 81.90%\n",
      "Epoch 2, Batch 1400/1474, Loss: 0.6294, Training Accuracy: 81.92%\n",
      "Epoch 2, Batch 1410/1474, Loss: 0.4986, Training Accuracy: 81.96%\n",
      "Epoch 2, Batch 1420/1474, Loss: 0.5238, Training Accuracy: 81.99%\n",
      "Epoch 2, Batch 1430/1474, Loss: 0.5255, Training Accuracy: 82.02%\n",
      "Epoch 2, Batch 1440/1474, Loss: 0.5575, Training Accuracy: 82.05%\n",
      "Epoch 2, Batch 1450/1474, Loss: 0.5952, Training Accuracy: 82.07%\n",
      "Epoch 2, Batch 1460/1474, Loss: 0.5664, Training Accuracy: 82.10%\n",
      "Epoch 2, Batch 1470/1474, Loss: 0.5479, Training Accuracy: 82.13%\n",
      "Epoch 2, Training Accuracy: 82.13%, Validation Accuracy: 89.79%\n",
      "Epoch 3, Batch 10/1474, Loss: 0.3193, Training Accuracy: 91.41%\n",
      "Epoch 3, Batch 20/1474, Loss: 0.3850, Training Accuracy: 91.17%\n",
      "Epoch 3, Batch 30/1474, Loss: 0.3676, Training Accuracy: 91.46%\n",
      "Epoch 3, Batch 40/1474, Loss: 0.3297, Training Accuracy: 91.52%\n",
      "Epoch 3, Batch 50/1474, Loss: 0.3721, Training Accuracy: 91.19%\n",
      "Epoch 3, Batch 60/1474, Loss: 0.3574, Training Accuracy: 91.25%\n",
      "Epoch 3, Batch 70/1474, Loss: 0.2834, Training Accuracy: 91.65%\n",
      "Epoch 3, Batch 80/1474, Loss: 0.2778, Training Accuracy: 91.88%\n",
      "Epoch 3, Batch 90/1474, Loss: 0.3069, Training Accuracy: 91.96%\n",
      "Epoch 3, Batch 100/1474, Loss: 0.2996, Training Accuracy: 91.91%\n",
      "Epoch 3, Batch 110/1474, Loss: 0.2957, Training Accuracy: 91.92%\n",
      "Epoch 3, Batch 120/1474, Loss: 0.2786, Training Accuracy: 92.01%\n",
      "Epoch 3, Batch 130/1474, Loss: 0.3215, Training Accuracy: 92.00%\n",
      "Epoch 3, Batch 140/1474, Loss: 0.3315, Training Accuracy: 91.95%\n",
      "Epoch 3, Batch 150/1474, Loss: 0.2828, Training Accuracy: 91.94%\n",
      "Epoch 3, Batch 160/1474, Loss: 0.3164, Training Accuracy: 91.90%\n",
      "Epoch 3, Batch 170/1474, Loss: 0.2296, Training Accuracy: 92.00%\n",
      "Epoch 3, Batch 180/1474, Loss: 0.2603, Training Accuracy: 92.03%\n",
      "Epoch 3, Batch 190/1474, Loss: 0.3070, Training Accuracy: 92.05%\n",
      "Epoch 3, Batch 200/1474, Loss: 0.3717, Training Accuracy: 92.01%\n",
      "Epoch 3, Batch 210/1474, Loss: 0.2885, Training Accuracy: 92.04%\n",
      "Epoch 3, Batch 220/1474, Loss: 0.2799, Training Accuracy: 92.11%\n",
      "Epoch 3, Batch 230/1474, Loss: 0.3205, Training Accuracy: 92.10%\n",
      "Epoch 3, Batch 240/1474, Loss: 0.3263, Training Accuracy: 92.04%\n",
      "Epoch 3, Batch 250/1474, Loss: 0.2265, Training Accuracy: 92.12%\n",
      "Epoch 3, Batch 260/1474, Loss: 0.2750, Training Accuracy: 92.13%\n",
      "Epoch 3, Batch 270/1474, Loss: 0.3056, Training Accuracy: 92.13%\n",
      "Epoch 3, Batch 280/1474, Loss: 0.2532, Training Accuracy: 92.19%\n",
      "Epoch 3, Batch 290/1474, Loss: 0.2987, Training Accuracy: 92.22%\n",
      "Epoch 3, Batch 300/1474, Loss: 0.2866, Training Accuracy: 92.21%\n",
      "Epoch 3, Batch 310/1474, Loss: 0.2285, Training Accuracy: 92.24%\n",
      "Epoch 3, Batch 320/1474, Loss: 0.3159, Training Accuracy: 92.23%\n",
      "Epoch 3, Batch 330/1474, Loss: 0.2995, Training Accuracy: 92.23%\n",
      "Epoch 3, Batch 340/1474, Loss: 0.2575, Training Accuracy: 92.25%\n",
      "Epoch 3, Batch 350/1474, Loss: 0.2605, Training Accuracy: 92.29%\n",
      "Epoch 3, Batch 360/1474, Loss: 0.2605, Training Accuracy: 92.34%\n",
      "Epoch 3, Batch 370/1474, Loss: 0.2904, Training Accuracy: 92.33%\n",
      "Epoch 3, Batch 380/1474, Loss: 0.2320, Training Accuracy: 92.36%\n",
      "Epoch 3, Batch 390/1474, Loss: 0.2617, Training Accuracy: 92.36%\n",
      "Epoch 3, Batch 400/1474, Loss: 0.2858, Training Accuracy: 92.34%\n",
      "Epoch 3, Batch 410/1474, Loss: 0.2467, Training Accuracy: 92.38%\n",
      "Epoch 3, Batch 420/1474, Loss: 0.2636, Training Accuracy: 92.40%\n",
      "Epoch 3, Batch 430/1474, Loss: 0.2786, Training Accuracy: 92.42%\n",
      "Epoch 3, Batch 440/1474, Loss: 0.2499, Training Accuracy: 92.45%\n",
      "Epoch 3, Batch 450/1474, Loss: 0.2738, Training Accuracy: 92.45%\n",
      "Epoch 3, Batch 460/1474, Loss: 0.2643, Training Accuracy: 92.46%\n",
      "Epoch 3, Batch 470/1474, Loss: 0.3301, Training Accuracy: 92.41%\n",
      "Epoch 3, Batch 480/1474, Loss: 0.2753, Training Accuracy: 92.39%\n",
      "Epoch 3, Batch 490/1474, Loss: 0.2311, Training Accuracy: 92.40%\n",
      "Epoch 3, Batch 500/1474, Loss: 0.2214, Training Accuracy: 92.41%\n",
      "Epoch 3, Batch 510/1474, Loss: 0.2786, Training Accuracy: 92.41%\n",
      "Epoch 3, Batch 520/1474, Loss: 0.3046, Training Accuracy: 92.41%\n",
      "Epoch 3, Batch 530/1474, Loss: 0.2532, Training Accuracy: 92.43%\n",
      "Epoch 3, Batch 540/1474, Loss: 0.3324, Training Accuracy: 92.41%\n",
      "Epoch 3, Batch 550/1474, Loss: 0.2049, Training Accuracy: 92.46%\n",
      "Epoch 3, Batch 560/1474, Loss: 0.2830, Training Accuracy: 92.45%\n",
      "Epoch 3, Batch 570/1474, Loss: 0.2348, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 580/1474, Loss: 0.3102, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 590/1474, Loss: 0.2781, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 600/1474, Loss: 0.2991, Training Accuracy: 92.47%\n",
      "Epoch 3, Batch 610/1474, Loss: 0.2555, Training Accuracy: 92.47%\n",
      "Epoch 3, Batch 620/1474, Loss: 0.2339, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 630/1474, Loss: 0.2257, Training Accuracy: 92.53%\n",
      "Epoch 3, Batch 640/1474, Loss: 0.2799, Training Accuracy: 92.54%\n",
      "Epoch 3, Batch 650/1474, Loss: 0.2758, Training Accuracy: 92.55%\n",
      "Epoch 3, Batch 660/1474, Loss: 0.2957, Training Accuracy: 92.54%\n",
      "Epoch 3, Batch 670/1474, Loss: 0.2485, Training Accuracy: 92.55%\n",
      "Epoch 3, Batch 680/1474, Loss: 0.2595, Training Accuracy: 92.56%\n",
      "Epoch 3, Batch 690/1474, Loss: 0.2785, Training Accuracy: 92.56%\n",
      "Epoch 3, Batch 700/1474, Loss: 0.2869, Training Accuracy: 92.56%\n",
      "Epoch 3, Batch 710/1474, Loss: 0.2551, Training Accuracy: 92.58%\n",
      "Epoch 3, Batch 720/1474, Loss: 0.2637, Training Accuracy: 92.58%\n",
      "Epoch 3, Batch 730/1474, Loss: 0.1877, Training Accuracy: 92.62%\n",
      "Epoch 3, Batch 740/1474, Loss: 0.2446, Training Accuracy: 92.64%\n",
      "Epoch 3, Batch 750/1474, Loss: 0.2629, Training Accuracy: 92.64%\n",
      "Epoch 3, Batch 760/1474, Loss: 0.2679, Training Accuracy: 92.65%\n",
      "Epoch 3, Batch 770/1474, Loss: 0.2829, Training Accuracy: 92.65%\n",
      "Epoch 3, Batch 780/1474, Loss: 0.3306, Training Accuracy: 92.62%\n",
      "Epoch 3, Batch 790/1474, Loss: 0.2365, Training Accuracy: 92.62%\n",
      "Epoch 3, Batch 800/1474, Loss: 0.2773, Training Accuracy: 92.61%\n",
      "Epoch 3, Batch 810/1474, Loss: 0.2664, Training Accuracy: 92.61%\n",
      "Epoch 3, Batch 820/1474, Loss: 0.2629, Training Accuracy: 92.61%\n",
      "Epoch 3, Batch 830/1474, Loss: 0.2606, Training Accuracy: 92.61%\n",
      "Epoch 3, Batch 840/1474, Loss: 0.2888, Training Accuracy: 92.62%\n",
      "Epoch 3, Batch 850/1474, Loss: 0.3373, Training Accuracy: 92.59%\n",
      "Epoch 3, Batch 860/1474, Loss: 0.3367, Training Accuracy: 92.56%\n",
      "Epoch 3, Batch 870/1474, Loss: 0.3284, Training Accuracy: 92.55%\n",
      "Epoch 3, Batch 880/1474, Loss: 0.3291, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 890/1474, Loss: 0.3111, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 900/1474, Loss: 0.3092, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 910/1474, Loss: 0.2898, Training Accuracy: 92.47%\n",
      "Epoch 3, Batch 920/1474, Loss: 0.3034, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 930/1474, Loss: 0.2105, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 940/1474, Loss: 0.2923, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 950/1474, Loss: 0.2226, Training Accuracy: 92.52%\n",
      "Epoch 3, Batch 960/1474, Loss: 0.2517, Training Accuracy: 92.53%\n",
      "Epoch 3, Batch 970/1474, Loss: 0.2776, Training Accuracy: 92.52%\n",
      "Epoch 3, Batch 980/1474, Loss: 0.3025, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 990/1474, Loss: 0.3226, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1000/1474, Loss: 0.2283, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1010/1474, Loss: 0.2425, Training Accuracy: 92.53%\n",
      "Epoch 3, Batch 1020/1474, Loss: 0.2660, Training Accuracy: 92.53%\n",
      "Epoch 3, Batch 1030/1474, Loss: 0.3361, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1040/1474, Loss: 0.2689, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1050/1474, Loss: 0.2794, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1060/1474, Loss: 0.2681, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1070/1474, Loss: 0.2966, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1080/1474, Loss: 0.3042, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1090/1474, Loss: 0.3316, Training Accuracy: 92.47%\n",
      "Epoch 3, Batch 1100/1474, Loss: 0.2780, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 1110/1474, Loss: 0.3159, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 1120/1474, Loss: 0.3062, Training Accuracy: 92.47%\n",
      "Epoch 3, Batch 1130/1474, Loss: 0.2714, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 1140/1474, Loss: 0.2914, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 1150/1474, Loss: 0.2422, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1160/1474, Loss: 0.2674, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1170/1474, Loss: 0.2679, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1180/1474, Loss: 0.2375, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1190/1474, Loss: 0.3331, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1200/1474, Loss: 0.2829, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1210/1474, Loss: 0.2393, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1220/1474, Loss: 0.2361, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1230/1474, Loss: 0.2863, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1240/1474, Loss: 0.3080, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1250/1474, Loss: 0.2687, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1260/1474, Loss: 0.2912, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1270/1474, Loss: 0.2878, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1280/1474, Loss: 0.2648, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1290/1474, Loss: 0.3241, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1300/1474, Loss: 0.2991, Training Accuracy: 92.49%\n",
      "Epoch 3, Batch 1310/1474, Loss: 0.3161, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 1320/1474, Loss: 0.2958, Training Accuracy: 92.48%\n",
      "Epoch 3, Batch 1330/1474, Loss: 0.1766, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1340/1474, Loss: 0.2537, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1350/1474, Loss: 0.2886, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1360/1474, Loss: 0.2539, Training Accuracy: 92.52%\n",
      "Epoch 3, Batch 1370/1474, Loss: 0.3335, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1380/1474, Loss: 0.3135, Training Accuracy: 92.50%\n",
      "Epoch 3, Batch 1390/1474, Loss: 0.2576, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1400/1474, Loss: 0.2759, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1410/1474, Loss: 0.2650, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1420/1474, Loss: 0.2870, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1430/1474, Loss: 0.2365, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1440/1474, Loss: 0.2880, Training Accuracy: 92.51%\n",
      "Epoch 3, Batch 1450/1474, Loss: 0.1833, Training Accuracy: 92.53%\n",
      "Epoch 3, Batch 1460/1474, Loss: 0.3168, Training Accuracy: 92.52%\n",
      "Epoch 3, Batch 1470/1474, Loss: 0.2918, Training Accuracy: 92.51%\n",
      "Epoch 3, Training Accuracy: 92.51%, Validation Accuracy: 96.28%\n",
      "Epoch 4, Batch 10/1474, Loss: 0.1637, Training Accuracy: 95.94%\n",
      "Epoch 4, Batch 20/1474, Loss: 0.1499, Training Accuracy: 96.25%\n",
      "Epoch 4, Batch 30/1474, Loss: 0.1241, Training Accuracy: 96.56%\n",
      "Epoch 4, Batch 40/1474, Loss: 0.1194, Training Accuracy: 96.56%\n",
      "Epoch 4, Batch 50/1474, Loss: 0.1083, Training Accuracy: 96.72%\n",
      "Epoch 4, Batch 60/1474, Loss: 0.1182, Training Accuracy: 96.90%\n",
      "Epoch 4, Batch 70/1474, Loss: 0.1000, Training Accuracy: 96.99%\n",
      "Epoch 4, Batch 80/1474, Loss: 0.1472, Training Accuracy: 96.88%\n",
      "Epoch 4, Batch 90/1474, Loss: 0.1242, Training Accuracy: 96.84%\n",
      "Epoch 4, Batch 100/1474, Loss: 0.0995, Training Accuracy: 96.92%\n",
      "Epoch 4, Batch 110/1474, Loss: 0.1316, Training Accuracy: 96.89%\n",
      "Epoch 4, Batch 120/1474, Loss: 0.0986, Training Accuracy: 96.95%\n",
      "Epoch 4, Batch 130/1474, Loss: 0.1064, Training Accuracy: 97.00%\n",
      "Epoch 4, Batch 140/1474, Loss: 0.1276, Training Accuracy: 96.96%\n",
      "Epoch 4, Batch 150/1474, Loss: 0.0888, Training Accuracy: 97.05%\n",
      "Epoch 4, Batch 160/1474, Loss: 0.0986, Training Accuracy: 97.08%\n",
      "Epoch 4, Batch 170/1474, Loss: 0.1148, Training Accuracy: 97.10%\n",
      "Epoch 4, Batch 180/1474, Loss: 0.0987, Training Accuracy: 97.12%\n",
      "Epoch 4, Batch 190/1474, Loss: 0.0865, Training Accuracy: 97.15%\n",
      "Epoch 4, Batch 200/1474, Loss: 0.0915, Training Accuracy: 97.16%\n",
      "Epoch 4, Batch 210/1474, Loss: 0.1011, Training Accuracy: 97.19%\n",
      "Epoch 4, Batch 220/1474, Loss: 0.1076, Training Accuracy: 97.19%\n",
      "Epoch 4, Batch 230/1474, Loss: 0.0939, Training Accuracy: 97.23%\n",
      "Epoch 4, Batch 240/1474, Loss: 0.0711, Training Accuracy: 97.29%\n",
      "Epoch 4, Batch 250/1474, Loss: 0.0900, Training Accuracy: 97.31%\n",
      "Epoch 4, Batch 260/1474, Loss: 0.0945, Training Accuracy: 97.31%\n",
      "Epoch 4, Batch 270/1474, Loss: 0.1170, Training Accuracy: 97.31%\n",
      "Epoch 4, Batch 280/1474, Loss: 0.0760, Training Accuracy: 97.35%\n",
      "Epoch 4, Batch 290/1474, Loss: 0.0970, Training Accuracy: 97.36%\n",
      "Epoch 4, Batch 300/1474, Loss: 0.1314, Training Accuracy: 97.33%\n",
      "Epoch 4, Batch 310/1474, Loss: 0.0776, Training Accuracy: 97.36%\n",
      "Epoch 4, Batch 320/1474, Loss: 0.0793, Training Accuracy: 97.40%\n",
      "Epoch 4, Batch 330/1474, Loss: 0.0963, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 340/1474, Loss: 0.1107, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 350/1474, Loss: 0.0710, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 360/1474, Loss: 0.0839, Training Accuracy: 97.47%\n",
      "Epoch 4, Batch 370/1474, Loss: 0.1073, Training Accuracy: 97.47%\n",
      "Epoch 4, Batch 380/1474, Loss: 0.1108, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 390/1474, Loss: 0.1190, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 400/1474, Loss: 0.0842, Training Accuracy: 97.48%\n",
      "Epoch 4, Batch 410/1474, Loss: 0.0908, Training Accuracy: 97.49%\n",
      "Epoch 4, Batch 420/1474, Loss: 0.0928, Training Accuracy: 97.49%\n",
      "Epoch 4, Batch 430/1474, Loss: 0.0956, Training Accuracy: 97.49%\n",
      "Epoch 4, Batch 440/1474, Loss: 0.0823, Training Accuracy: 97.51%\n",
      "Epoch 4, Batch 450/1474, Loss: 0.0879, Training Accuracy: 97.52%\n",
      "Epoch 4, Batch 460/1474, Loss: 0.0557, Training Accuracy: 97.55%\n",
      "Epoch 4, Batch 470/1474, Loss: 0.0799, Training Accuracy: 97.55%\n",
      "Epoch 4, Batch 480/1474, Loss: 0.1130, Training Accuracy: 97.53%\n",
      "Epoch 4, Batch 490/1474, Loss: 0.0808, Training Accuracy: 97.54%\n",
      "Epoch 4, Batch 500/1474, Loss: 0.0696, Training Accuracy: 97.56%\n",
      "Epoch 4, Batch 510/1474, Loss: 0.1111, Training Accuracy: 97.55%\n",
      "Epoch 4, Batch 520/1474, Loss: 0.0997, Training Accuracy: 97.54%\n",
      "Epoch 4, Batch 530/1474, Loss: 0.1058, Training Accuracy: 97.53%\n",
      "Epoch 4, Batch 540/1474, Loss: 0.0568, Training Accuracy: 97.54%\n",
      "Epoch 4, Batch 550/1474, Loss: 0.0904, Training Accuracy: 97.53%\n",
      "Epoch 4, Batch 560/1474, Loss: 0.0683, Training Accuracy: 97.55%\n",
      "Epoch 4, Batch 570/1474, Loss: 0.0841, Training Accuracy: 97.55%\n",
      "Epoch 4, Batch 580/1474, Loss: 0.1161, Training Accuracy: 97.54%\n",
      "Epoch 4, Batch 590/1474, Loss: 0.1068, Training Accuracy: 97.52%\n",
      "Epoch 4, Batch 600/1474, Loss: 0.1125, Training Accuracy: 97.51%\n",
      "Epoch 4, Batch 610/1474, Loss: 0.0902, Training Accuracy: 97.51%\n",
      "Epoch 4, Batch 620/1474, Loss: 0.0983, Training Accuracy: 97.51%\n",
      "Epoch 4, Batch 630/1474, Loss: 0.1156, Training Accuracy: 97.50%\n",
      "Epoch 4, Batch 640/1474, Loss: 0.1118, Training Accuracy: 97.49%\n",
      "Epoch 4, Batch 650/1474, Loss: 0.0753, Training Accuracy: 97.50%\n",
      "Epoch 4, Batch 660/1474, Loss: 0.1200, Training Accuracy: 97.49%\n",
      "Epoch 4, Batch 670/1474, Loss: 0.1167, Training Accuracy: 97.48%\n",
      "Epoch 4, Batch 680/1474, Loss: 0.1029, Training Accuracy: 97.48%\n",
      "Epoch 4, Batch 690/1474, Loss: 0.0973, Training Accuracy: 97.48%\n",
      "Epoch 4, Batch 700/1474, Loss: 0.1116, Training Accuracy: 97.47%\n",
      "Epoch 4, Batch 710/1474, Loss: 0.0970, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 720/1474, Loss: 0.1011, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 730/1474, Loss: 0.0879, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 740/1474, Loss: 0.1149, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 750/1474, Loss: 0.0962, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 760/1474, Loss: 0.1036, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 770/1474, Loss: 0.1107, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 780/1474, Loss: 0.1067, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 790/1474, Loss: 0.0982, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 800/1474, Loss: 0.0985, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 810/1474, Loss: 0.1110, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 820/1474, Loss: 0.1008, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 830/1474, Loss: 0.0720, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 840/1474, Loss: 0.1167, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 850/1474, Loss: 0.0757, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 860/1474, Loss: 0.0826, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 870/1474, Loss: 0.1057, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 880/1474, Loss: 0.1011, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 890/1474, Loss: 0.0731, Training Accuracy: 97.46%\n",
      "Epoch 4, Batch 900/1474, Loss: 0.1267, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 910/1474, Loss: 0.1140, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 920/1474, Loss: 0.0800, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 930/1474, Loss: 0.0909, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 940/1474, Loss: 0.1177, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 950/1474, Loss: 0.0981, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 960/1474, Loss: 0.1121, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 970/1474, Loss: 0.1203, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 980/1474, Loss: 0.0800, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 990/1474, Loss: 0.0937, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 1000/1474, Loss: 0.1230, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1010/1474, Loss: 0.0955, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1020/1474, Loss: 0.0954, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1030/1474, Loss: 0.0748, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1040/1474, Loss: 0.1108, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 1050/1474, Loss: 0.1020, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1060/1474, Loss: 0.0807, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1070/1474, Loss: 0.0922, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 1080/1474, Loss: 0.1083, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1090/1474, Loss: 0.0903, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1100/1474, Loss: 0.0927, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 1110/1474, Loss: 0.0692, Training Accuracy: 97.45%\n",
      "Epoch 4, Batch 1120/1474, Loss: 0.0951, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 1130/1474, Loss: 0.1216, Training Accuracy: 97.44%\n",
      "Epoch 4, Batch 1140/1474, Loss: 0.1035, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1150/1474, Loss: 0.1155, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1160/1474, Loss: 0.1105, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1170/1474, Loss: 0.1164, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1180/1474, Loss: 0.1015, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1190/1474, Loss: 0.0861, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1200/1474, Loss: 0.0951, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1210/1474, Loss: 0.0959, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1220/1474, Loss: 0.0927, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1230/1474, Loss: 0.1075, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1240/1474, Loss: 0.1022, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1250/1474, Loss: 0.0884, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1260/1474, Loss: 0.0733, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1270/1474, Loss: 0.1233, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1280/1474, Loss: 0.1099, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1290/1474, Loss: 0.1203, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1300/1474, Loss: 0.0883, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1310/1474, Loss: 0.1065, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1320/1474, Loss: 0.0917, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1330/1474, Loss: 0.0953, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1340/1474, Loss: 0.1072, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1350/1474, Loss: 0.0811, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1360/1474, Loss: 0.0902, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1370/1474, Loss: 0.1050, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1380/1474, Loss: 0.1138, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1390/1474, Loss: 0.0860, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1400/1474, Loss: 0.0955, Training Accuracy: 97.43%\n",
      "Epoch 4, Batch 1410/1474, Loss: 0.1043, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1420/1474, Loss: 0.1216, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1430/1474, Loss: 0.1086, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1440/1474, Loss: 0.0894, Training Accuracy: 97.42%\n",
      "Epoch 4, Batch 1450/1474, Loss: 0.1453, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1460/1474, Loss: 0.1000, Training Accuracy: 97.41%\n",
      "Epoch 4, Batch 1470/1474, Loss: 0.1179, Training Accuracy: 97.40%\n",
      "Epoch 4, Training Accuracy: 97.40%, Validation Accuracy: 98.94%\n",
      "Epoch 5, Batch 10/1474, Loss: 0.0496, Training Accuracy: 98.75%\n",
      "Epoch 5, Batch 20/1474, Loss: 0.0559, Training Accuracy: 98.98%\n",
      "Epoch 5, Batch 30/1474, Loss: 0.0496, Training Accuracy: 98.85%\n",
      "Epoch 5, Batch 40/1474, Loss: 0.0505, Training Accuracy: 98.83%\n",
      "Epoch 5, Batch 50/1474, Loss: 0.0360, Training Accuracy: 98.97%\n",
      "Epoch 5, Batch 60/1474, Loss: 0.0434, Training Accuracy: 98.96%\n",
      "Epoch 5, Batch 70/1474, Loss: 0.0566, Training Accuracy: 98.91%\n",
      "Epoch 5, Batch 80/1474, Loss: 0.0394, Training Accuracy: 98.96%\n",
      "Epoch 5, Batch 90/1474, Loss: 0.0380, Training Accuracy: 98.96%\n",
      "Epoch 5, Batch 100/1474, Loss: 0.0335, Training Accuracy: 99.02%\n",
      "Epoch 5, Batch 110/1474, Loss: 0.0250, Training Accuracy: 99.11%\n",
      "Epoch 5, Batch 120/1474, Loss: 0.0294, Training Accuracy: 99.11%\n",
      "Epoch 5, Batch 130/1474, Loss: 0.0332, Training Accuracy: 99.13%\n",
      "Epoch 5, Batch 140/1474, Loss: 0.0639, Training Accuracy: 99.08%\n",
      "Epoch 5, Batch 150/1474, Loss: 0.0335, Training Accuracy: 99.11%\n",
      "Epoch 5, Batch 160/1474, Loss: 0.0278, Training Accuracy: 99.15%\n",
      "Epoch 5, Batch 170/1474, Loss: 0.0445, Training Accuracy: 99.15%\n",
      "Epoch 5, Batch 180/1474, Loss: 0.0247, Training Accuracy: 99.19%\n",
      "Epoch 5, Batch 190/1474, Loss: 0.0263, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 200/1474, Loss: 0.0282, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 210/1474, Loss: 0.0351, Training Accuracy: 99.23%\n",
      "Epoch 5, Batch 220/1474, Loss: 0.0279, Training Accuracy: 99.25%\n",
      "Epoch 5, Batch 230/1474, Loss: 0.0262, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 240/1474, Loss: 0.0333, Training Accuracy: 99.27%\n",
      "Epoch 5, Batch 250/1474, Loss: 0.0379, Training Accuracy: 99.26%\n",
      "Epoch 5, Batch 260/1474, Loss: 0.0246, Training Accuracy: 99.28%\n",
      "Epoch 5, Batch 270/1474, Loss: 0.0264, Training Accuracy: 99.30%\n",
      "Epoch 5, Batch 280/1474, Loss: 0.0350, Training Accuracy: 99.30%\n",
      "Epoch 5, Batch 290/1474, Loss: 0.0234, Training Accuracy: 99.31%\n",
      "Epoch 5, Batch 300/1474, Loss: 0.0239, Training Accuracy: 99.32%\n",
      "Epoch 5, Batch 310/1474, Loss: 0.0253, Training Accuracy: 99.34%\n",
      "Epoch 5, Batch 320/1474, Loss: 0.0271, Training Accuracy: 99.35%\n",
      "Epoch 5, Batch 330/1474, Loss: 0.0283, Training Accuracy: 99.35%\n",
      "Epoch 5, Batch 340/1474, Loss: 0.0297, Training Accuracy: 99.36%\n",
      "Epoch 5, Batch 350/1474, Loss: 0.0340, Training Accuracy: 99.35%\n",
      "Epoch 5, Batch 360/1474, Loss: 0.0329, Training Accuracy: 99.35%\n",
      "Epoch 5, Batch 370/1474, Loss: 0.0248, Training Accuracy: 99.35%\n",
      "Epoch 5, Batch 380/1474, Loss: 0.0265, Training Accuracy: 99.35%\n",
      "Epoch 5, Batch 390/1474, Loss: 0.0316, Training Accuracy: 99.35%\n",
      "Epoch 5, Batch 400/1474, Loss: 0.0328, Training Accuracy: 99.36%\n",
      "Epoch 5, Batch 410/1474, Loss: 0.0214, Training Accuracy: 99.37%\n",
      "Epoch 5, Batch 420/1474, Loss: 0.0186, Training Accuracy: 99.38%\n",
      "Epoch 5, Batch 430/1474, Loss: 0.0342, Training Accuracy: 99.38%\n",
      "Epoch 5, Batch 440/1474, Loss: 0.0291, Training Accuracy: 99.38%\n",
      "Epoch 5, Batch 450/1474, Loss: 0.0270, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 460/1474, Loss: 0.0301, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 470/1474, Loss: 0.0277, Training Accuracy: 99.38%\n",
      "Epoch 5, Batch 480/1474, Loss: 0.0281, Training Accuracy: 99.38%\n",
      "Epoch 5, Batch 490/1474, Loss: 0.0177, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 500/1474, Loss: 0.0202, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 510/1474, Loss: 0.0413, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 520/1474, Loss: 0.0169, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 530/1474, Loss: 0.0205, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 540/1474, Loss: 0.0464, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 550/1474, Loss: 0.0226, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 560/1474, Loss: 0.0210, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 570/1474, Loss: 0.0274, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 580/1474, Loss: 0.0294, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 590/1474, Loss: 0.0261, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 600/1474, Loss: 0.0196, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 610/1474, Loss: 0.0228, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 620/1474, Loss: 0.0248, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 630/1474, Loss: 0.0292, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 640/1474, Loss: 0.0227, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 650/1474, Loss: 0.0275, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 660/1474, Loss: 0.0276, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 670/1474, Loss: 0.0230, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 680/1474, Loss: 0.0187, Training Accuracy: 99.44%\n",
      "Epoch 5, Batch 690/1474, Loss: 0.0323, Training Accuracy: 99.44%\n",
      "Epoch 5, Batch 700/1474, Loss: 0.0374, Training Accuracy: 99.44%\n",
      "Epoch 5, Batch 710/1474, Loss: 0.0303, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 720/1474, Loss: 0.0205, Training Accuracy: 99.44%\n",
      "Epoch 5, Batch 730/1474, Loss: 0.0287, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 740/1474, Loss: 0.0333, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 750/1474, Loss: 0.0357, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 760/1474, Loss: 0.0260, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 770/1474, Loss: 0.0318, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 780/1474, Loss: 0.0306, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 790/1474, Loss: 0.0292, Training Accuracy: 99.43%\n",
      "Epoch 5, Batch 800/1474, Loss: 0.0335, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 810/1474, Loss: 0.0470, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 820/1474, Loss: 0.0231, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 830/1474, Loss: 0.0451, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 840/1474, Loss: 0.0315, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 850/1474, Loss: 0.0237, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 860/1474, Loss: 0.0412, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 870/1474, Loss: 0.0291, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 880/1474, Loss: 0.0302, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 890/1474, Loss: 0.0204, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 900/1474, Loss: 0.0301, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 910/1474, Loss: 0.0219, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 920/1474, Loss: 0.0287, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 930/1474, Loss: 0.0291, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 940/1474, Loss: 0.0327, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 950/1474, Loss: 0.0327, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 960/1474, Loss: 0.0239, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 970/1474, Loss: 0.0242, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 980/1474, Loss: 0.0311, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 990/1474, Loss: 0.0393, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1000/1474, Loss: 0.0265, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1010/1474, Loss: 0.0197, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1020/1474, Loss: 0.0380, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1030/1474, Loss: 0.0320, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1040/1474, Loss: 0.0202, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1050/1474, Loss: 0.0235, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1060/1474, Loss: 0.0236, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1070/1474, Loss: 0.0327, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1080/1474, Loss: 0.0313, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1090/1474, Loss: 0.0532, Training Accuracy: 99.42%\n",
      "Epoch 5, Batch 1100/1474, Loss: 0.0289, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1110/1474, Loss: 0.0347, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1120/1474, Loss: 0.0304, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1130/1474, Loss: 0.0238, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1140/1474, Loss: 0.0408, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1150/1474, Loss: 0.0436, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1160/1474, Loss: 0.0273, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1170/1474, Loss: 0.0508, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1180/1474, Loss: 0.0170, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1190/1474, Loss: 0.0302, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1200/1474, Loss: 0.0292, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1210/1474, Loss: 0.0242, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1220/1474, Loss: 0.0170, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1230/1474, Loss: 0.0397, Training Accuracy: 99.41%\n",
      "Epoch 5, Batch 1240/1474, Loss: 0.0321, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1250/1474, Loss: 0.0225, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1260/1474, Loss: 0.0301, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1270/1474, Loss: 0.0353, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1280/1474, Loss: 0.0356, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1290/1474, Loss: 0.0354, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1300/1474, Loss: 0.0237, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1310/1474, Loss: 0.0242, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1320/1474, Loss: 0.0390, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1330/1474, Loss: 0.0297, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1340/1474, Loss: 0.0296, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1350/1474, Loss: 0.0443, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1360/1474, Loss: 0.0319, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1370/1474, Loss: 0.0325, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1380/1474, Loss: 0.0377, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 1390/1474, Loss: 0.0321, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 1400/1474, Loss: 0.0175, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 1410/1474, Loss: 0.0235, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1420/1474, Loss: 0.0164, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1430/1474, Loss: 0.0284, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1440/1474, Loss: 0.0338, Training Accuracy: 99.40%\n",
      "Epoch 5, Batch 1450/1474, Loss: 0.0417, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 1460/1474, Loss: 0.0292, Training Accuracy: 99.39%\n",
      "Epoch 5, Batch 1470/1474, Loss: 0.0202, Training Accuracy: 99.39%\n",
      "Epoch 5, Training Accuracy: 99.39%, Validation Accuracy: 99.67%\n",
      "Epoch 6, Batch 10/1474, Loss: 0.0446, Training Accuracy: 98.91%\n",
      "Epoch 6, Batch 20/1474, Loss: 0.0317, Training Accuracy: 99.30%\n",
      "Epoch 6, Batch 30/1474, Loss: 0.0098, Training Accuracy: 99.48%\n",
      "Epoch 6, Batch 40/1474, Loss: 0.0183, Training Accuracy: 99.53%\n",
      "Epoch 6, Batch 50/1474, Loss: 0.0084, Training Accuracy: 99.62%\n",
      "Epoch 6, Batch 60/1474, Loss: 0.0100, Training Accuracy: 99.66%\n",
      "Epoch 6, Batch 70/1474, Loss: 0.0152, Training Accuracy: 99.64%\n",
      "Epoch 6, Batch 80/1474, Loss: 0.0089, Training Accuracy: 99.69%\n",
      "Epoch 6, Batch 90/1474, Loss: 0.0092, Training Accuracy: 99.72%\n",
      "Epoch 6, Batch 100/1474, Loss: 0.0099, Training Accuracy: 99.75%\n",
      "Epoch 6, Batch 110/1474, Loss: 0.0124, Training Accuracy: 99.74%\n",
      "Epoch 6, Batch 120/1474, Loss: 0.0228, Training Accuracy: 99.74%\n",
      "Epoch 6, Batch 130/1474, Loss: 0.0067, Training Accuracy: 99.76%\n",
      "Epoch 6, Batch 140/1474, Loss: 0.0131, Training Accuracy: 99.77%\n",
      "Epoch 6, Batch 150/1474, Loss: 0.0125, Training Accuracy: 99.76%\n",
      "Epoch 6, Batch 160/1474, Loss: 0.0075, Training Accuracy: 99.78%\n",
      "Epoch 6, Batch 170/1474, Loss: 0.0081, Training Accuracy: 99.79%\n",
      "Epoch 6, Batch 180/1474, Loss: 0.0085, Training Accuracy: 99.79%\n",
      "Epoch 6, Batch 190/1474, Loss: 0.0091, Training Accuracy: 99.79%\n",
      "Epoch 6, Batch 200/1474, Loss: 0.0124, Training Accuracy: 99.79%\n",
      "Epoch 6, Batch 210/1474, Loss: 0.0064, Training Accuracy: 99.80%\n",
      "Epoch 6, Batch 220/1474, Loss: 0.0152, Training Accuracy: 99.79%\n",
      "Epoch 6, Batch 230/1474, Loss: 0.0080, Training Accuracy: 99.80%\n",
      "Epoch 6, Batch 240/1474, Loss: 0.0145, Training Accuracy: 99.80%\n",
      "Epoch 6, Batch 250/1474, Loss: 0.0092, Training Accuracy: 99.81%\n",
      "Epoch 6, Batch 260/1474, Loss: 0.0082, Training Accuracy: 99.81%\n",
      "Epoch 6, Batch 270/1474, Loss: 0.0191, Training Accuracy: 99.80%\n",
      "Epoch 6, Batch 280/1474, Loss: 0.0077, Training Accuracy: 99.81%\n",
      "Epoch 6, Batch 290/1474, Loss: 0.0109, Training Accuracy: 99.81%\n",
      "Epoch 6, Batch 300/1474, Loss: 0.0074, Training Accuracy: 99.81%\n",
      "Epoch 6, Batch 310/1474, Loss: 0.0054, Training Accuracy: 99.82%\n",
      "Epoch 6, Batch 320/1474, Loss: 0.0143, Training Accuracy: 99.81%\n",
      "Epoch 6, Batch 330/1474, Loss: 0.0068, Training Accuracy: 99.82%\n",
      "Epoch 6, Batch 340/1474, Loss: 0.0070, Training Accuracy: 99.83%\n",
      "Epoch 6, Batch 350/1474, Loss: 0.0097, Training Accuracy: 99.83%\n",
      "Epoch 6, Batch 360/1474, Loss: 0.0089, Training Accuracy: 99.83%\n",
      "Epoch 6, Batch 370/1474, Loss: 0.0070, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 380/1474, Loss: 0.0084, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 390/1474, Loss: 0.0055, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 400/1474, Loss: 0.0131, Training Accuracy: 99.83%\n",
      "Epoch 6, Batch 410/1474, Loss: 0.0079, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 420/1474, Loss: 0.0064, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 430/1474, Loss: 0.0147, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 440/1474, Loss: 0.0064, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 450/1474, Loss: 0.0074, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 460/1474, Loss: 0.0119, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 470/1474, Loss: 0.0117, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 480/1474, Loss: 0.0053, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 490/1474, Loss: 0.0086, Training Accuracy: 99.85%\n",
      "Epoch 6, Batch 500/1474, Loss: 0.0103, Training Accuracy: 99.85%\n",
      "Epoch 6, Batch 510/1474, Loss: 0.0111, Training Accuracy: 99.84%\n",
      "Epoch 6, Batch 520/1474, Loss: 0.0064, Training Accuracy: 99.85%\n",
      "Epoch 6, Batch 530/1474, Loss: 0.0073, Training Accuracy: 99.85%\n",
      "Epoch 6, Batch 540/1474, Loss: 0.0068, Training Accuracy: 99.85%\n",
      "Epoch 6, Batch 550/1474, Loss: 0.0078, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 560/1474, Loss: 0.0080, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 570/1474, Loss: 0.0059, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 580/1474, Loss: 0.0117, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 590/1474, Loss: 0.0039, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 600/1474, Loss: 0.0069, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 610/1474, Loss: 0.0047, Training Accuracy: 99.86%\n",
      "Epoch 6, Batch 620/1474, Loss: 0.0079, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 630/1474, Loss: 0.0066, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 640/1474, Loss: 0.0051, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 650/1474, Loss: 0.0046, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 660/1474, Loss: 0.0087, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 670/1474, Loss: 0.0052, Training Accuracy: 99.87%\n",
      "Epoch 6, Batch 680/1474, Loss: 0.0051, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 690/1474, Loss: 0.0062, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 700/1474, Loss: 0.0051, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 710/1474, Loss: 0.0043, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 720/1474, Loss: 0.0054, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 730/1474, Loss: 0.0054, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 740/1474, Loss: 0.0084, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 750/1474, Loss: 0.0059, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 760/1474, Loss: 0.0051, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 770/1474, Loss: 0.0045, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 780/1474, Loss: 0.0090, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 790/1474, Loss: 0.0068, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 800/1474, Loss: 0.0048, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 810/1474, Loss: 0.0054, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 820/1474, Loss: 0.0083, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 830/1474, Loss: 0.0144, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 840/1474, Loss: 0.0085, Training Accuracy: 99.88%\n",
      "Epoch 6, Batch 850/1474, Loss: 0.0046, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 860/1474, Loss: 0.0055, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 870/1474, Loss: 0.0065, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 880/1474, Loss: 0.0071, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 890/1474, Loss: 0.0068, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 900/1474, Loss: 0.0075, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 910/1474, Loss: 0.0059, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 920/1474, Loss: 0.0056, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 930/1474, Loss: 0.0056, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 940/1474, Loss: 0.0095, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 950/1474, Loss: 0.0060, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 960/1474, Loss: 0.0057, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 970/1474, Loss: 0.0061, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 980/1474, Loss: 0.0094, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 990/1474, Loss: 0.0053, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1000/1474, Loss: 0.0078, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1010/1474, Loss: 0.0091, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1020/1474, Loss: 0.0125, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1030/1474, Loss: 0.0061, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1040/1474, Loss: 0.0065, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1050/1474, Loss: 0.0060, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1060/1474, Loss: 0.0055, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1070/1474, Loss: 0.0052, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1080/1474, Loss: 0.0107, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1090/1474, Loss: 0.0051, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1100/1474, Loss: 0.0093, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1110/1474, Loss: 0.0076, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1120/1474, Loss: 0.0142, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1130/1474, Loss: 0.0070, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1140/1474, Loss: 0.0094, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1150/1474, Loss: 0.0072, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1160/1474, Loss: 0.0062, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1170/1474, Loss: 0.0076, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1180/1474, Loss: 0.0074, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1190/1474, Loss: 0.0062, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1200/1474, Loss: 0.0073, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1210/1474, Loss: 0.0052, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1220/1474, Loss: 0.0042, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1230/1474, Loss: 0.0066, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1240/1474, Loss: 0.0105, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1250/1474, Loss: 0.0093, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1260/1474, Loss: 0.0071, Training Accuracy: 99.90%\n",
      "Epoch 6, Batch 1270/1474, Loss: 0.0114, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1280/1474, Loss: 0.0099, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1290/1474, Loss: 0.0100, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1300/1474, Loss: 0.0061, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1310/1474, Loss: 0.0202, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1320/1474, Loss: 0.0064, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1330/1474, Loss: 0.0111, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1340/1474, Loss: 0.0060, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1350/1474, Loss: 0.0075, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1360/1474, Loss: 0.0065, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1370/1474, Loss: 0.0091, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1380/1474, Loss: 0.0039, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1390/1474, Loss: 0.0197, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1400/1474, Loss: 0.0062, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1410/1474, Loss: 0.0150, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1420/1474, Loss: 0.0075, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1430/1474, Loss: 0.0079, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1440/1474, Loss: 0.0218, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1450/1474, Loss: 0.0091, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1460/1474, Loss: 0.0181, Training Accuracy: 99.89%\n",
      "Epoch 6, Batch 1470/1474, Loss: 0.0104, Training Accuracy: 99.89%\n",
      "Epoch 6, Training Accuracy: 99.89%, Validation Accuracy: 99.93%\n",
      "Epoch 7, Batch 10/1474, Loss: 0.0054, Training Accuracy: 100.00%\n",
      "Epoch 7, Batch 20/1474, Loss: 0.0031, Training Accuracy: 100.00%\n",
      "Epoch 7, Batch 30/1474, Loss: 0.0062, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 40/1474, Loss: 0.0049, Training Accuracy: 99.96%\n",
      "Epoch 7, Batch 50/1474, Loss: 0.0047, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 60/1474, Loss: 0.0032, Training Accuracy: 99.97%\n",
      "Epoch 7, Batch 70/1474, Loss: 0.0040, Training Accuracy: 99.96%\n",
      "Epoch 7, Batch 80/1474, Loss: 0.0083, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 90/1474, Loss: 0.0049, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 100/1474, Loss: 0.0122, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 110/1474, Loss: 0.0066, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 120/1474, Loss: 0.0073, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 130/1474, Loss: 0.0177, Training Accuracy: 99.89%\n",
      "Epoch 7, Batch 140/1474, Loss: 0.0081, Training Accuracy: 99.88%\n",
      "Epoch 7, Batch 150/1474, Loss: 0.0092, Training Accuracy: 99.85%\n",
      "Epoch 7, Batch 160/1474, Loss: 0.0058, Training Accuracy: 99.85%\n",
      "Epoch 7, Batch 170/1474, Loss: 0.0037, Training Accuracy: 99.86%\n",
      "Epoch 7, Batch 180/1474, Loss: 0.0032, Training Accuracy: 99.87%\n",
      "Epoch 7, Batch 190/1474, Loss: 0.0115, Training Accuracy: 99.87%\n",
      "Epoch 7, Batch 200/1474, Loss: 0.0027, Training Accuracy: 99.88%\n",
      "Epoch 7, Batch 210/1474, Loss: 0.0058, Training Accuracy: 99.87%\n",
      "Epoch 7, Batch 220/1474, Loss: 0.0036, Training Accuracy: 99.88%\n",
      "Epoch 7, Batch 230/1474, Loss: 0.0035, Training Accuracy: 99.88%\n",
      "Epoch 7, Batch 240/1474, Loss: 0.0054, Training Accuracy: 99.88%\n",
      "Epoch 7, Batch 250/1474, Loss: 0.0028, Training Accuracy: 99.89%\n",
      "Epoch 7, Batch 260/1474, Loss: 0.0048, Training Accuracy: 99.89%\n",
      "Epoch 7, Batch 270/1474, Loss: 0.0031, Training Accuracy: 99.89%\n",
      "Epoch 7, Batch 280/1474, Loss: 0.0108, Training Accuracy: 99.89%\n",
      "Epoch 7, Batch 290/1474, Loss: 0.0026, Training Accuracy: 99.89%\n",
      "Epoch 7, Batch 300/1474, Loss: 0.0033, Training Accuracy: 99.90%\n",
      "Epoch 7, Batch 310/1474, Loss: 0.0049, Training Accuracy: 99.90%\n",
      "Epoch 7, Batch 320/1474, Loss: 0.0034, Training Accuracy: 99.90%\n",
      "Epoch 7, Batch 330/1474, Loss: 0.0039, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 340/1474, Loss: 0.0031, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 350/1474, Loss: 0.0034, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 360/1474, Loss: 0.0029, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 370/1474, Loss: 0.0044, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 380/1474, Loss: 0.0038, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 390/1474, Loss: 0.0068, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 400/1474, Loss: 0.0036, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 410/1474, Loss: 0.0053, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 420/1474, Loss: 0.0055, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 430/1474, Loss: 0.0066, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 440/1474, Loss: 0.0098, Training Accuracy: 99.90%\n",
      "Epoch 7, Batch 450/1474, Loss: 0.0035, Training Accuracy: 99.90%\n",
      "Epoch 7, Batch 460/1474, Loss: 0.0036, Training Accuracy: 99.90%\n",
      "Epoch 7, Batch 470/1474, Loss: 0.0036, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 480/1474, Loss: 0.0025, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 490/1474, Loss: 0.0036, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 500/1474, Loss: 0.0051, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 510/1474, Loss: 0.0113, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 520/1474, Loss: 0.0029, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 530/1474, Loss: 0.0028, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 540/1474, Loss: 0.0031, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 550/1474, Loss: 0.0027, Training Accuracy: 99.91%\n",
      "Epoch 7, Batch 560/1474, Loss: 0.0027, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 570/1474, Loss: 0.0020, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 580/1474, Loss: 0.0020, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 590/1474, Loss: 0.0030, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 600/1474, Loss: 0.0040, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 610/1474, Loss: 0.0026, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 620/1474, Loss: 0.0022, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 630/1474, Loss: 0.0029, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 640/1474, Loss: 0.0025, Training Accuracy: 99.92%\n",
      "Epoch 7, Batch 650/1474, Loss: 0.0021, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 660/1474, Loss: 0.0032, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 670/1474, Loss: 0.0036, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 680/1474, Loss: 0.0019, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 690/1474, Loss: 0.0025, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 700/1474, Loss: 0.0023, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 710/1474, Loss: 0.0031, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 720/1474, Loss: 0.0027, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 730/1474, Loss: 0.0037, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 740/1474, Loss: 0.0033, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 750/1474, Loss: 0.0031, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 760/1474, Loss: 0.0052, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 770/1474, Loss: 0.0033, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 780/1474, Loss: 0.0029, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 790/1474, Loss: 0.0027, Training Accuracy: 99.93%\n",
      "Epoch 7, Batch 800/1474, Loss: 0.0026, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 810/1474, Loss: 0.0018, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 820/1474, Loss: 0.0025, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 830/1474, Loss: 0.0041, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 840/1474, Loss: 0.0020, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 850/1474, Loss: 0.0025, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 860/1474, Loss: 0.0049, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 870/1474, Loss: 0.0033, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 880/1474, Loss: 0.0025, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 890/1474, Loss: 0.0035, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 900/1474, Loss: 0.0023, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 910/1474, Loss: 0.0022, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 920/1474, Loss: 0.0057, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 930/1474, Loss: 0.0073, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 940/1474, Loss: 0.0031, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 950/1474, Loss: 0.0025, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 960/1474, Loss: 0.0022, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 970/1474, Loss: 0.0062, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 980/1474, Loss: 0.0022, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 990/1474, Loss: 0.0040, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 1000/1474, Loss: 0.0028, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 1010/1474, Loss: 0.0028, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 1020/1474, Loss: 0.0034, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 1030/1474, Loss: 0.0022, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 1040/1474, Loss: 0.0026, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 1050/1474, Loss: 0.0025, Training Accuracy: 99.94%\n",
      "Epoch 7, Batch 1060/1474, Loss: 0.0028, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1070/1474, Loss: 0.0026, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1080/1474, Loss: 0.0051, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1090/1474, Loss: 0.0020, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1100/1474, Loss: 0.0043, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1110/1474, Loss: 0.0022, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1120/1474, Loss: 0.0028, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1130/1474, Loss: 0.0028, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1140/1474, Loss: 0.0026, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1150/1474, Loss: 0.0016, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1160/1474, Loss: 0.0059, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1170/1474, Loss: 0.0017, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1180/1474, Loss: 0.0064, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1190/1474, Loss: 0.0088, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1200/1474, Loss: 0.0023, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1210/1474, Loss: 0.0019, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1220/1474, Loss: 0.0032, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1230/1474, Loss: 0.0027, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1240/1474, Loss: 0.0026, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1250/1474, Loss: 0.0022, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1260/1474, Loss: 0.0090, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1270/1474, Loss: 0.0018, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1280/1474, Loss: 0.0020, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1290/1474, Loss: 0.0020, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1300/1474, Loss: 0.0024, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1310/1474, Loss: 0.0015, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1320/1474, Loss: 0.0021, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1330/1474, Loss: 0.0019, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1340/1474, Loss: 0.0027, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1350/1474, Loss: 0.0023, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1360/1474, Loss: 0.0027, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1370/1474, Loss: 0.0086, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1380/1474, Loss: 0.0035, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1390/1474, Loss: 0.0033, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1400/1474, Loss: 0.0091, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1410/1474, Loss: 0.0024, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1420/1474, Loss: 0.0025, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1430/1474, Loss: 0.0023, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1440/1474, Loss: 0.0029, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1450/1474, Loss: 0.0020, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1460/1474, Loss: 0.0027, Training Accuracy: 99.95%\n",
      "Epoch 7, Batch 1470/1474, Loss: 0.0017, Training Accuracy: 99.95%\n",
      "Epoch 7, Training Accuracy: 99.95%, Validation Accuracy: 99.97%\n",
      "Epoch 8, Batch 10/1474, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 20/1474, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 30/1474, Loss: 0.0017, Training Accuracy: 100.00%\n",
      "Epoch 8, Batch 40/1474, Loss: 0.0032, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 50/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 60/1474, Loss: 0.0096, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 70/1474, Loss: 0.0014, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 80/1474, Loss: 0.0014, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 90/1474, Loss: 0.0021, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 100/1474, Loss: 0.0015, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 110/1474, Loss: 0.0014, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 120/1474, Loss: 0.0021, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 130/1474, Loss: 0.0011, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 140/1474, Loss: 0.0014, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 150/1474, Loss: 0.0015, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 160/1474, Loss: 0.0015, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 170/1474, Loss: 0.0017, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 180/1474, Loss: 0.0014, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 190/1474, Loss: 0.0017, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 200/1474, Loss: 0.0016, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 210/1474, Loss: 0.0015, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 220/1474, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 230/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 240/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 250/1474, Loss: 0.0013, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 260/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 270/1474, Loss: 0.0014, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 280/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 290/1474, Loss: 0.0024, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 300/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 310/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 320/1474, Loss: 0.0016, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 330/1474, Loss: 0.0039, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 340/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 8, Batch 350/1474, Loss: 0.0045, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 360/1474, Loss: 0.0050, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 370/1474, Loss: 0.0016, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 380/1474, Loss: 0.0014, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 390/1474, Loss: 0.0013, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 400/1474, Loss: 0.0012, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 410/1474, Loss: 0.0016, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 420/1474, Loss: 0.0014, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 430/1474, Loss: 0.0012, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 440/1474, Loss: 0.0017, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 450/1474, Loss: 0.0030, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 460/1474, Loss: 0.0037, Training Accuracy: 99.98%\n",
      "Epoch 8, Batch 470/1474, Loss: 0.0072, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 480/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 490/1474, Loss: 0.0017, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 500/1474, Loss: 0.0024, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 510/1474, Loss: 0.0056, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 520/1474, Loss: 0.0015, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 530/1474, Loss: 0.0013, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 540/1474, Loss: 0.0021, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 550/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 560/1474, Loss: 0.0026, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 570/1474, Loss: 0.0060, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 580/1474, Loss: 0.0026, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 590/1474, Loss: 0.0125, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 600/1474, Loss: 0.0038, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 610/1474, Loss: 0.0018, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 620/1474, Loss: 0.0018, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 630/1474, Loss: 0.0019, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 640/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 650/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 660/1474, Loss: 0.0097, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 670/1474, Loss: 0.0020, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 680/1474, Loss: 0.0053, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 690/1474, Loss: 0.0014, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 700/1474, Loss: 0.0015, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 710/1474, Loss: 0.0016, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 720/1474, Loss: 0.0017, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 730/1474, Loss: 0.0015, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 740/1474, Loss: 0.0050, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 750/1474, Loss: 0.0015, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 760/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 770/1474, Loss: 0.0024, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 780/1474, Loss: 0.0011, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 790/1474, Loss: 0.0013, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 800/1474, Loss: 0.0014, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 810/1474, Loss: 0.0018, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 820/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 830/1474, Loss: 0.0020, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 840/1474, Loss: 0.0013, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 850/1474, Loss: 0.0015, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 860/1474, Loss: 0.0020, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 870/1474, Loss: 0.0017, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 880/1474, Loss: 0.0014, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 890/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 900/1474, Loss: 0.0013, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 910/1474, Loss: 0.0011, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 920/1474, Loss: 0.0041, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 930/1474, Loss: 0.0016, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 940/1474, Loss: 0.0017, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 950/1474, Loss: 0.0012, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 960/1474, Loss: 0.0013, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 970/1474, Loss: 0.0020, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 980/1474, Loss: 0.0015, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 990/1474, Loss: 0.0044, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1000/1474, Loss: 0.0013, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1010/1474, Loss: 0.0012, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1020/1474, Loss: 0.0012, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1030/1474, Loss: 0.0015, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1040/1474, Loss: 0.0014, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1050/1474, Loss: 0.0018, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1060/1474, Loss: 0.0014, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1070/1474, Loss: 0.0060, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1080/1474, Loss: 0.0111, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1090/1474, Loss: 0.0089, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1100/1474, Loss: 0.0026, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1110/1474, Loss: 0.0054, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1120/1474, Loss: 0.0054, Training Accuracy: 99.97%\n",
      "Epoch 8, Batch 1130/1474, Loss: 0.0156, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1140/1474, Loss: 0.0030, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1150/1474, Loss: 0.0020, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1160/1474, Loss: 0.0033, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1170/1474, Loss: 0.0161, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1180/1474, Loss: 0.0070, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1190/1474, Loss: 0.0054, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1200/1474, Loss: 0.0125, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1210/1474, Loss: 0.0140, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1220/1474, Loss: 0.0019, Training Accuracy: 99.96%\n",
      "Epoch 8, Batch 1230/1474, Loss: 0.0205, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1240/1474, Loss: 0.0070, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1250/1474, Loss: 0.0022, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1260/1474, Loss: 0.0050, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1270/1474, Loss: 0.0049, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1280/1474, Loss: 0.0036, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1290/1474, Loss: 0.0054, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1300/1474, Loss: 0.0015, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1310/1474, Loss: 0.0034, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1320/1474, Loss: 0.0089, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1330/1474, Loss: 0.0055, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1340/1474, Loss: 0.0022, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1350/1474, Loss: 0.0016, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1360/1474, Loss: 0.0033, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1370/1474, Loss: 0.0026, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1380/1474, Loss: 0.0018, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1390/1474, Loss: 0.0016, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1400/1474, Loss: 0.0019, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1410/1474, Loss: 0.0026, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1420/1474, Loss: 0.0018, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1430/1474, Loss: 0.0018, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1440/1474, Loss: 0.0017, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1450/1474, Loss: 0.0019, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1460/1474, Loss: 0.0096, Training Accuracy: 99.95%\n",
      "Epoch 8, Batch 1470/1474, Loss: 0.0017, Training Accuracy: 99.95%\n",
      "Epoch 8, Training Accuracy: 99.95%, Validation Accuracy: 100.00%\n",
      "Epoch 9, Batch 10/1474, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 20/1474, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 30/1474, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 40/1474, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 50/1474, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 60/1474, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 70/1474, Loss: 0.0010, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 80/1474, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 9, Batch 90/1474, Loss: 0.0030, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 100/1474, Loss: 0.0009, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 110/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 120/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 130/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 140/1474, Loss: 0.0014, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 150/1474, Loss: 0.0013, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 160/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 170/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 180/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 190/1474, Loss: 0.0015, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 200/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 210/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 220/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 230/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 240/1474, Loss: 0.0013, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 250/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 260/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 270/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 280/1474, Loss: 0.0022, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 290/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 300/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 310/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 320/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 330/1474, Loss: 0.0057, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 340/1474, Loss: 0.0023, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 350/1474, Loss: 0.0013, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 360/1474, Loss: 0.0018, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 370/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 380/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 390/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 400/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 410/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 420/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 430/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 440/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 450/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 460/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 470/1474, Loss: 0.0027, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 480/1474, Loss: 0.0014, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 490/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 500/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 510/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 520/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 530/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 540/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 550/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 560/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 570/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 580/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 590/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 600/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 610/1474, Loss: 0.0035, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 620/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 630/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 640/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 650/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 660/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 670/1474, Loss: 0.0062, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 680/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 690/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 700/1474, Loss: 0.0035, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 710/1474, Loss: 0.0010, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 720/1474, Loss: 0.0014, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 730/1474, Loss: 0.0017, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 740/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 750/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 760/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 770/1474, Loss: 0.0013, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 780/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 790/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 800/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 810/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 820/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 830/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 840/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 850/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 860/1474, Loss: 0.0013, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 870/1474, Loss: 0.0023, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 880/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 890/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 900/1474, Loss: 0.0089, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 910/1474, Loss: 0.0046, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 920/1474, Loss: 0.0009, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 930/1474, Loss: 0.0009, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 940/1474, Loss: 0.0007, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 950/1474, Loss: 0.0009, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 960/1474, Loss: 0.0009, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 970/1474, Loss: 0.0011, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 980/1474, Loss: 0.0009, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 990/1474, Loss: 0.0009, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 1000/1474, Loss: 0.0007, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 1010/1474, Loss: 0.0010, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 1020/1474, Loss: 0.0008, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 1030/1474, Loss: 0.0008, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 1040/1474, Loss: 0.0010, Training Accuracy: 99.98%\n",
      "Epoch 9, Batch 1050/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1060/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1070/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1080/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1090/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1100/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1110/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1120/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1130/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1140/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1150/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1160/1474, Loss: 0.0011, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1170/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1180/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1190/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1200/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1210/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1220/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1230/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1240/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1250/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1260/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1270/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1280/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1290/1474, Loss: 0.0013, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1300/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1310/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1320/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1330/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1340/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1350/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1360/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1370/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1380/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1390/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1400/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1410/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1420/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1430/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1440/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1450/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1460/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 9, Batch 1470/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 9, Training Accuracy: 99.99%, Validation Accuracy: 100.00%\n",
      "Epoch 10, Batch 10/1474, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 20/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 30/1474, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 40/1474, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 50/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 60/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 70/1474, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 80/1474, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 90/1474, Loss: 0.0007, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 100/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 110/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 120/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 130/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 140/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 150/1474, Loss: 0.0009, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 160/1474, Loss: 0.0008, Training Accuracy: 100.00%\n",
      "Epoch 10, Batch 170/1474, Loss: 0.0025, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 180/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 190/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 200/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 210/1474, Loss: 0.0033, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 220/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 230/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 240/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 250/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 260/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 270/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 280/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 290/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 300/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 310/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 320/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 330/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 340/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 350/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 360/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 370/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 380/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 390/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 400/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 410/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 420/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 430/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 440/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 450/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 460/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 470/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 480/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 490/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 500/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 510/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 520/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 530/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 540/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 550/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 560/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 570/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 580/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 590/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 600/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 610/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 620/1474, Loss: 0.0036, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 630/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 640/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 650/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 660/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 670/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 680/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 690/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 700/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 710/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 720/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 730/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 740/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 750/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 760/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 770/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 780/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 790/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 800/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 810/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 820/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 830/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 840/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 850/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 860/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 870/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 880/1474, Loss: 0.0108, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 890/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 900/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 910/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 920/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 930/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 940/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 950/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 960/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 970/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 980/1474, Loss: 0.0004, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 990/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1000/1474, Loss: 0.0015, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1010/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1020/1474, Loss: 0.0010, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1030/1474, Loss: 0.0044, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1040/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1050/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1060/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1070/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1080/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1090/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1100/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1110/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1120/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1130/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1140/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1150/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1160/1474, Loss: 0.0009, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1170/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1180/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1190/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1200/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1210/1474, Loss: 0.0012, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1220/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1230/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1240/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1250/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1260/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1270/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1280/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1290/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1300/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1310/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1320/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1330/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1340/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1350/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1360/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1370/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1380/1474, Loss: 0.0008, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1390/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1400/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1410/1474, Loss: 0.0005, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1420/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1430/1474, Loss: 0.0007, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1440/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1450/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1460/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Batch 1470/1474, Loss: 0.0006, Training Accuracy: 99.99%\n",
      "Epoch 10, Training Accuracy: 99.99%, Validation Accuracy: 100.00%\n",
      "Epoch 11, Batch 10/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 20/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 30/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 40/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 50/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 60/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 70/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 80/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 90/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 100/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 110/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 120/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 130/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 140/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 150/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 160/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 170/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 180/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 190/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 200/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 210/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 220/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 230/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 240/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 250/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 260/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 270/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 280/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 290/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 300/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 310/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 320/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 330/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 340/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 350/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 360/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 370/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 380/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 390/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 400/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 410/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 420/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 430/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 440/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 450/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 460/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 470/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 480/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 490/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 500/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 510/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 520/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 530/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 540/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 550/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 560/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 570/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 580/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 590/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 600/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 610/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 620/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 630/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 640/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 650/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 660/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 670/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 680/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 690/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 700/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 710/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 720/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 730/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 740/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 750/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 760/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 770/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 780/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 790/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 800/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 810/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 820/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 830/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 840/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 850/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 860/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 870/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 880/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 890/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 900/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 910/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 920/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 930/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 940/1474, Loss: 0.0058, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 950/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 960/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 970/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 980/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 990/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1000/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1010/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1020/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1030/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1040/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1050/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1060/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1070/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1080/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1090/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1100/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1110/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1120/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1130/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1140/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1150/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1160/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1170/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1180/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1190/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1200/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1210/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1220/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1230/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1240/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1250/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1260/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1270/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1280/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1290/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1300/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1310/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1320/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1330/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1340/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1350/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1360/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1370/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1380/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1390/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1400/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1410/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1420/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1430/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1440/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1450/1474, Loss: 0.0046, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1460/1474, Loss: 0.0005, Training Accuracy: 100.00%\n",
      "Epoch 11, Batch 1470/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 11, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 12, Batch 10/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 20/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 30/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 40/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 50/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 60/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 70/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 80/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 90/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 100/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 110/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 120/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 130/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 140/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 150/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 160/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 170/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 180/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 190/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 200/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 210/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 220/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 230/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 240/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 250/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 260/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 270/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 280/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 290/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 300/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 310/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 320/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 330/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 340/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 350/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 360/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 370/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 380/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 390/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 400/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 410/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 420/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 430/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 440/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 450/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 460/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 470/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 480/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 490/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 500/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 510/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 520/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 530/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 540/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 550/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 560/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 570/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 580/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 590/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 600/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 610/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 620/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 630/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 640/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 650/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 660/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 670/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 680/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 690/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 700/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 710/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 720/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 730/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 740/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 750/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 760/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 770/1474, Loss: 0.0080, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 780/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 790/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 800/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 810/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 820/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 830/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 840/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 850/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 860/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 870/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 880/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 890/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 900/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 910/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 920/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 930/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 940/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 950/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 960/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 970/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 980/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 990/1474, Loss: 0.0033, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1000/1474, Loss: 0.0030, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1010/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1020/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1030/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1040/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1050/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1060/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1070/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1080/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1090/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1100/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1110/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1120/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1130/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1140/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1150/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1160/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1170/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1180/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1190/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1200/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1210/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1220/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1230/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1240/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1250/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1260/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1270/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1280/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1290/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1300/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1310/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1320/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1330/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1340/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1350/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1360/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1370/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1380/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1390/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1400/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1410/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1420/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1430/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1440/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1450/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1460/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 12, Batch 1470/1474, Loss: 0.0004, Training Accuracy: 100.00%\n",
      "Epoch 12, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 13, Batch 10/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 20/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 30/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 40/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 50/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 60/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 70/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 80/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 90/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 100/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 110/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 120/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 130/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 140/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 150/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 160/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 170/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 180/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 190/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 200/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 210/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 220/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 230/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 240/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 250/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 260/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 270/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 280/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 290/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 300/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 310/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 320/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 330/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 340/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 350/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 360/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 370/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 380/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 390/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 400/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 410/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 420/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 430/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 440/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 450/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 460/1474, Loss: 0.0059, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 470/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 480/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 490/1474, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 500/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 510/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 520/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 530/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 540/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 550/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 560/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 570/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 580/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 590/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 600/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 610/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 620/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 630/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 640/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 650/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 660/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 670/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 680/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 690/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 700/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 710/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 720/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 730/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 740/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 750/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 760/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 770/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 780/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 790/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 800/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 810/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 820/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 830/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 840/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 850/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 860/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 870/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 880/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 890/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 900/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 910/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 920/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 930/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 940/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 950/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 960/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 970/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 980/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 990/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1000/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1010/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1020/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1030/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1040/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1050/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1060/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1070/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1080/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1090/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1100/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1110/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1120/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1130/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1140/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1150/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1160/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1170/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1180/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1190/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1200/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1210/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1220/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1230/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1240/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1250/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1260/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1270/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1280/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1290/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1300/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1310/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1320/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1330/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1340/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1350/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1360/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1370/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1380/1474, Loss: 0.0003, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1390/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1400/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1410/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1420/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1430/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1440/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1450/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1460/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Batch 1470/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 13, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 14, Batch 10/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 20/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 30/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 40/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 50/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 60/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 70/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 80/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 90/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 100/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 110/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 120/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 130/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 140/1474, Loss: 0.0029, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 150/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 160/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 170/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 180/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 190/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 200/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 210/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 220/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 230/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 240/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 250/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 260/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 270/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 280/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 290/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 300/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 310/1474, Loss: 0.0002, Training Accuracy: 99.99%\n",
      "Epoch 14, Batch 320/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 330/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 340/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 350/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 360/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 370/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 380/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 390/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 400/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 410/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 420/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 430/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 440/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 450/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 460/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 470/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 480/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 490/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 500/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 510/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 520/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 530/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 540/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 550/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 560/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 570/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 580/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 590/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 600/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 610/1474, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 620/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 630/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 640/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 650/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 660/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 670/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 680/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 690/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 700/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 710/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 720/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 730/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 740/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 750/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 760/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 770/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 780/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 790/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 800/1474, Loss: 0.0035, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 810/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 820/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 830/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 840/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 850/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 860/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 870/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 880/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 890/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 900/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 910/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 920/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 930/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 940/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 950/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 960/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 970/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 980/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 990/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1000/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1010/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1020/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1030/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1040/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1050/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1060/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1070/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1080/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1090/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1100/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1110/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1120/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1130/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1150/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1160/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1170/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1180/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1190/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1200/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1210/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1220/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1250/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1260/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1270/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1280/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1290/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1300/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1310/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1320/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1330/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1340/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1350/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1370/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1390/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1400/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1410/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1430/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1440/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1450/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1460/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 14, Batch 1470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 14, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 15, Batch 10/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 20/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 30/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 40/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 50/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 60/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 70/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 80/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 90/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 110/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 120/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 130/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 150/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 180/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 200/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 220/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 240/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 290/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 300/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 320/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 330/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 350/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 420/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 430/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 480/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 490/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 500/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 510/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 520/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 530/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 540/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 550/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 560/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 570/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 580/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 590/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 600/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 610/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 620/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 630/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 640/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 650/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 660/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 670/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 680/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 690/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 700/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 710/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 720/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 730/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 740/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 750/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 760/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 770/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 780/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 790/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 800/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 810/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 820/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 830/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 840/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 850/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 860/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 870/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 880/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 890/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 900/1474, Loss: 0.0012, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 910/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 920/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 930/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 940/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 950/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 960/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 970/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 980/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 990/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1000/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1010/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1020/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1030/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1040/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1050/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1060/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1070/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1080/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1090/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1100/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1180/1474, Loss: 0.0045, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1270/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1410/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1420/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1460/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 15, Batch 1470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 15, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 16, Batch 10/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 20/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 30/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 40/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 50/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 60/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 70/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 80/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 90/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 200/1474, Loss: 0.0018, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 210/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 220/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 230/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 240/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 250/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 260/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 270/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 280/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 290/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 300/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 310/1474, Loss: 0.0001, Training Accuracy: 99.99%\n",
      "Epoch 16, Batch 320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 480/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 490/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 500/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 510/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 520/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 530/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 540/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 550/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 560/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 570/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 580/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 590/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 600/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 610/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 620/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 630/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 640/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 650/1474, Loss: 0.0023, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 660/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 670/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 680/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 690/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 700/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 710/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 720/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 730/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 740/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 750/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 760/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 770/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 780/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 790/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 800/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 810/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 820/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 830/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 840/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 850/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 860/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 870/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 880/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 890/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 900/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 910/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 920/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 930/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 940/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 950/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 960/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 970/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 980/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 990/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1000/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1010/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1020/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1030/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1040/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1050/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1060/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1070/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1080/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1090/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1230/1474, Loss: 0.0006, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Batch 1470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 16, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 17, Batch 10/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 20/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 30/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 40/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 50/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 60/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 70/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 80/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 90/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 220/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 400/1474, Loss: 0.0011, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 480/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 490/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 500/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 510/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 520/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 530/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 540/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 550/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 560/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 570/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 580/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 590/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 600/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 610/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 620/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 630/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 640/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 650/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 660/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 670/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 680/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 690/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 700/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 710/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 720/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 730/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 740/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 750/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 760/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 770/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 780/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 790/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 800/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 810/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 820/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 830/1474, Loss: 0.0029, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 840/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 850/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 860/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 870/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 880/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 890/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 900/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 910/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 920/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 930/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 940/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 950/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 960/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 970/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 980/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 990/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1000/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1010/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1020/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1030/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1040/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1050/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1060/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1070/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1080/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1090/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Batch 1470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 17, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 18, Batch 10/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 20/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 30/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 40/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 50/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 60/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 70/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 80/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 90/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 210/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 480/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 490/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 500/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 510/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 520/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 530/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 540/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 550/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 560/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 570/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 580/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 590/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 600/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 610/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 620/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 630/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 640/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 650/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 660/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 670/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 680/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 690/1474, Loss: 0.0014, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 700/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 710/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 720/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 730/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 740/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 750/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 760/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 770/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 780/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 790/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 800/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 810/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 820/1474, Loss: 0.0022, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 830/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 840/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 850/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 860/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 870/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 880/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 890/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 900/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 910/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 920/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 930/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 940/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 950/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 960/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 970/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 980/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 990/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1000/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1010/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1020/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1030/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1040/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1050/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1060/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1070/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1080/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1090/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Batch 1470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 18, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 19, Batch 10/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 20/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 30/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 40/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 50/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 60/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 70/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 80/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 90/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 480/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 490/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 500/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 510/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 520/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 530/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 540/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 550/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 560/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 570/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 580/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 590/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 600/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 610/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 620/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 630/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 640/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 650/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 660/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 670/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 680/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 690/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 700/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 710/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 720/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 730/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 740/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 750/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 760/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 770/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 780/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 790/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 800/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 810/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 820/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 830/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 840/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 850/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 860/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 870/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 880/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 890/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 900/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 910/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 920/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 930/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 940/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 950/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 960/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 970/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 980/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 990/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1000/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1010/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1020/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1030/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1040/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1050/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1060/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1070/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1080/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1090/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1160/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1260/1474, Loss: 0.0029, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Batch 1470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 19, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Epoch 20, Batch 10/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 20/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 30/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 40/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 50/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 60/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 70/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 80/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 90/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 130/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 350/1474, Loss: 0.0002, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 480/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 490/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 500/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 510/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 520/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 530/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 540/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 550/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 560/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 570/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 580/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 590/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 600/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 610/1474, Loss: 0.0013, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 620/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 630/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 640/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 650/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 660/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 670/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 680/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 690/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 700/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 710/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 720/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 730/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 740/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 750/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 760/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 770/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 780/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 790/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 800/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 810/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 820/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 830/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 840/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 850/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 860/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 870/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 880/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 890/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 900/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 910/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 920/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 930/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 940/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 950/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 960/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 970/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 980/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 990/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1000/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1010/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1020/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1030/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1040/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1050/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1060/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1070/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1080/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1090/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1100/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1110/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1120/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1130/1474, Loss: 0.0017, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1140/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1150/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1160/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1170/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1180/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1190/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1200/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1210/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1220/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1230/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1240/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1250/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1260/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1270/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1280/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1290/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1300/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1310/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1320/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1330/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1340/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1350/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1360/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1370/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1380/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1390/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1400/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1410/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1420/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1430/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1440/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1450/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1460/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Batch 1470/1474, Loss: 0.0001, Training Accuracy: 100.00%\n",
      "Epoch 20, Training Accuracy: 100.00%, Validation Accuracy: 100.00%\n",
      "Test Accuracy: 91.41%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if batch_idx % 10 == 9:  # Print every 10 batches\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {running_loss / 10:.4f}, Training Accuracy: {100 * correct / total:.2f}%\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation accuracy calculation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Epoch {epoch+1}, Training Accuracy: {100 * correct / total:.2f}%, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_outputs = model(test_inputs)\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "        test_total += test_labels.size(0)\n",
    "        test_correct += (test_predicted == test_labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * test_correct / test_total:.2f}%\")\n",
    "torch.save(model.state_dict(), 'ViT_UCF101_pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
